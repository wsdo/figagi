{
	"title": "ðŸ¦œï¸ðŸ“ LangServe | ðŸ¦œï¸ðŸ”— Langchain",
	"url": "https://python.langchain.com/docs/langserve",
	"html": "Skip to main content\nðŸ¦œï¸ðŸ”— LangChain\nDocs\nUse cases\nIntegrations\nGuides\nAPI\nMore\nðŸ¦œï¸ðŸ”—\nChat\nSearch\nâŒ˜\nK\nGet started\nIntroduction\nInstallation\nQuickstart\nSecurity\nLangChain Expression Language\nGet started\nWhy use LCEL\nInterface\nHow to\nCookbook\nModules\nModel I/O\nRetrieval\nAgents\nMore\nLangServe\nLangSmith\nLangServe\nðŸ¦œï¸ðŸ“ LangServe\n\n   \n\nðŸš© We will be releasing a hosted version of LangServe for one-click deployments of LangChain applications. Sign up here to get on the waitlist.\n\nOverviewâ€‹\n\nLangServe helps developers deploy LangChain runnables and chains as a REST API.\n\nThis library is integrated with FastAPI and uses pydantic for data validation.\n\nIn addition, it provides a client that can be used to call into runnables deployed on a server. A javascript client is available in LangChainJS.\n\nFeaturesâ€‹\nInput and Output schemas automatically inferred from your LangChain object, and enforced on every API call, with rich error messages\nAPI docs page with JSONSchema and Swagger (insert example link)\nEfficient /invoke/, /batch/ and /stream/ endpoints with support for many concurrent requests on a single server\n/stream_log/ endpoint for streaming all (or some) intermediate steps from your chain/agent\nPlayground page at /playground/ with streaming output and intermediate steps\nBuilt-in (optional) tracing to LangSmith, just add your API key (see Instructions])\nAll built with battle-tested open-source Python libraries like FastAPI, Pydantic, uvloop and asyncio.\nUse the client SDK to call a LangServe server as if it was a Runnable running locally (or call the HTTP API directly)\nLangServe Hub\nLimitationsâ€‹\nClient callbacks are not yet supported for events that originate on the server\nOpenAPI docs will not be generated when using Pydantic V2. Fast API does not support mixing pydantic v1 and v2 namespaces. See section below for more details.\nHosted LangServeâ€‹\n\nWe will be releasing a hosted version of LangServe for one-click deployments of LangChain applications. Sign up here to get on the waitlist.\n\nSecurityâ€‹\nVulnerability in Versions 0.0.13 - 0.0.15 -- playground endpoint allows accessing arbitrary files on server. Resolved in 0.0.16.\nInstallationâ€‹\n\nFor both client and server:\n\npip install \"langserve[all]\"\n\n\nor pip install \"langserve[client]\" for client code, and pip install \"langserve[server]\" for server code.\n\nLangChain CLI ðŸ› ï¸â€‹\n\nUse the LangChain CLI to bootstrap a LangServe project quickly.\n\nTo use the langchain CLI make sure that you have a recent version of langchain-cli installed. You can install it with pip install -U langchain-cli.\n\nlangchain app new ../path/to/directory\n\nExamplesâ€‹\n\nGet your LangServe instance started quickly with LangChain Templates.\n\nFor more examples, see the templates index or the examples directory.\n\nServerâ€‹\n\nHere's a server that deploys an OpenAI chat model, an Anthropic chat model, and a chain that uses the Anthropic model to tell a joke about a topic.\n\n#!/usr/bin/env python\nfrom fastapi import FastAPI\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain.chat_models import ChatAnthropic, ChatOpenAI\nfrom langserve import add_routes\n\n\napp = FastAPI(\n  title=\"LangChain Server\",\n  version=\"1.0\",\n  description=\"A simple api server using Langchain's Runnable interfaces\",\n)\n\nadd_routes(\n    app,\n    ChatOpenAI(),\n    path=\"/openai\",\n)\n\nadd_routes(\n    app,\n    ChatAnthropic(),\n    path=\"/anthropic\",\n)\n\nmodel = ChatAnthropic()\nprompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\nadd_routes(\n    app,\n    prompt | model,\n    path=\"/joke\",\n)\n\nif __name__ == \"__main__\":\n    import uvicorn\n\n    uvicorn.run(app, host=\"localhost\", port=8000)\n\nDocsâ€‹\n\nIf you've deployed the server above, you can view the generated OpenAPI docs using:\n\nâš ï¸ If using pydantic v2, docs will not be generated for invoke, batch, stream, stream_log. See Pydantic section below for more details.\n\ncurl localhost:8000/docs\n\n\nmake sure to add the /docs suffix.\n\nâš ï¸ Index page / is not defined by design, so curl localhost:8000 or visiting the URL will return a 404. If you want content at / define an endpoint @app.get(\"/\").\n\nClientâ€‹\n\nPython SDK\n\n\nfrom langchain.schema import SystemMessage, HumanMessage\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain.schema.runnable import RunnableMap\nfrom langserve import RemoteRunnable\n\nopenai = RemoteRunnable(\"http://localhost:8000/openai/\")\nanthropic = RemoteRunnable(\"http://localhost:8000/anthropic/\")\njoke_chain = RemoteRunnable(\"http://localhost:8000/joke/\")\n\njoke_chain.invoke({\"topic\": \"parrots\"})\n\n# or async\nawait joke_chain.ainvoke({\"topic\": \"parrots\"})\n\nprompt = [\n    SystemMessage(content='Act like either a cat or a parrot.'),\n    HumanMessage(content='Hello!')\n]\n\n# Supports astream\nasync for msg in anthropic.astream(prompt):\n    print(msg, end=\"\", flush=True)\n\nprompt = ChatPromptTemplate.from_messages(\n    [(\"system\", \"Tell me a long story about {topic}\")]\n)\n\n# Can define custom chains\nchain = prompt | RunnableMap({\n    \"openai\": openai,\n    \"anthropic\": anthropic,\n})\n\nchain.batch([{ \"topic\": \"parrots\" }, { \"topic\": \"cats\" }])\n\n\nIn TypeScript (requires LangChain.js version 0.0.166 or later):\n\nimport { RemoteRunnable } from \"langchain/runnables/remote\";\n\nconst chain = new RemoteRunnable({\n  url: `http://localhost:8000/joke/`,\n});\nconst result = await chain.invoke({\n  topic: \"cats\",\n});\n\n\nPython using requests:\n\nimport requests\nresponse = requests.post(\n    \"http://localhost:8000/joke/invoke/\",\n    json={'input': {'topic': 'cats'}}\n)\nresponse.json()\n\n\nYou can also use curl:\n\ncurl --location --request POST 'http://localhost:8000/joke/invoke/' \\\n    --header 'Content-Type: application/json' \\\n    --data-raw '{\n        \"input\": {\n            \"topic\": \"cats\"\n        }\n    }'\n\nEndpointsâ€‹\n\nThe following code:\n\n...\nadd_routes(\n  app,\n  runnable,\n  path=\"/my_runnable\",\n)\n\n\nadds of these endpoints to the server:\n\nPOST /my_runnable/invoke - invoke the runnable on a single input\nPOST /my_runnable/batch - invoke the runnable on a batch of inputs\nPOST /my_runnable/stream - invoke on a single input and stream the output\nPOST /my_runnable/stream_log - invoke on a single input and stream the output, including output of intermediate steps as it's generated\nGET /my_runnable/input_schema - json schema for input to the runnable\nGET /my_runnable/output_schema - json schema for output of the runnable\nGET /my_runnable/config_schema - json schema for config of the runnable\n\nThese endpoints match the LangChain Expression Language interface -- please reference this documentation for more details.\n\nPlaygroundâ€‹\n\nYou can find a playground page for your runnable at /my_runnable/playground/. This exposes a simple UI to configure and invoke your runnable with streaming output and intermediate steps.\n\nWidgetsâ€‹\n\nThe playground supports widgets and can be used to test your runnable with different inputs.\n\nIn addition, for configurable runnables, the playground will allow you to configure the runnable and share a link with the configuration:\n\nSharingâ€‹\n\nLegacy Chainsâ€‹\n\nLangServe works with both Runnables (constructed via LangChain Expression Language) and legacy chains (inheriting from Chain). However, some of the input schemas for legacy chains may be incomplete/incorrect, leading to errors. This can be fixed by updating the input_schema property of those chains in LangChain. If you encounter any errors, please open an issue on THIS repo, and we will work to address it.\n\nDeploymentâ€‹\nDeploy to Azureâ€‹\n\nYou can deploy to Azure using Azure Container Apps (Serverless):\n\naz containerapp up --name [container-app-name] --source . --resource-group [resource-group-name] --environment  [environment-name] --ingress external --target-port 8001 --env-vars=OPENAI_API_KEY=your_key  \n\n\nYou can find more info here\n\nDeploy to GCPâ€‹\n\nYou can deploy to GCP Cloud Run using the following command:\n\ngcloud run deploy [your-service-name] --source . --port 8001 --allow-unauthenticated --region us-central1 --set-env-vars=OPENAI_API_KEY=your_key\n\nPydanticâ€‹\n\nLangServe provides support for Pydantic 2 with some limitations.\n\nOpenAPI docs will not be generated for invoke/batch/stream/stream_log when using Pydantic V2. Fast API does not support [mixing pydantic v1 and v2 namespaces].\nLangChain uses the v1 namespace in Pydantic v2. Please read the following guidelines to ensure compatibility with LangChain\n\nExcept for these limitations, we expect the API endpoints, the playground and any other features to work as expected.\n\nAdvancedâ€‹\nHandling Authenticationâ€‹\n\nIf you need to add authentication to your server, please reference FastAPI's security documentation and middleware documentation.\n\nFilesâ€‹\n\nLLM applications often deal with files. There are different architectures that can be made to implement file processing; at a high level:\n\nThe file may be uploaded to the server via a dedicated endpoint and processed using a separate endpoint\nThe file may be uploaded by either value (bytes of file) or reference (e.g., s3 url to file content)\nThe processing endpoint may be blocking or non-blocking\nIf significant processing is required, the processing may be offloaded to a dedicated process pool\n\nYou should determine what is the appropriate architecture for your application.\n\nCurrently, to upload files by value to a runnable, use base64 encoding for the file (multipart/form-data is not supported yet).\n\nHere's an example that shows how to use base64 encoding to send a file to a remote runnable.\n\nRemember, you can always upload files by reference (e.g., s3 url) or upload them as multipart/form-data to a dedicated endpoint.\n\nCustom Input and Output Typesâ€‹\n\nInput and Output types are defined on all runnables.\n\nYou can access them via the input_schema and output_schema properties.\n\nLangServe uses these types for validation and documentation.\n\nIf you want to override the default inferred types, you can use the with_types method.\n\nHere's a toy example to illustrate the idea:\n\nfrom typing import Any\n\nfrom fastapi import FastAPI\nfrom langchain.schema.runnable import RunnableLambda\n\napp = FastAPI()\n\n\ndef func(x: Any) -> int:\n    \"\"\"Mistyped function that should accept an int but accepts anything.\"\"\"\n    return x + 1\n\n\nrunnable = RunnableLambda(func).with_types(\n    input_schema=int,\n)\n\nadd_routes(app, runnable)\n\nCustom User Typesâ€‹\n\nInherit from CustomUserType if you want the data to de-serialize into a pydantic model rather than the equivalent dict representation.\n\nAt the moment, this type only works server side and is used to specify desired decoding behavior. If inheriting from this type the server will keep the decoded type as a pydantic model instead of converting it into a dict.\n\nfrom fastapi import FastAPI\nfrom langchain.schema.runnable import RunnableLambda\n\nfrom langserve import add_routes\nfrom langserve.schema import CustomUserType\n\napp = FastAPI()\n\n\nclass Foo(CustomUserType):\n    bar: int\n\n\ndef func(foo: Foo) -> int:\n    \"\"\"Sample function that expects a Foo type which is a pydantic model\"\"\"\n    assert isinstance(foo, Foo)\n    return foo.bar\n\n# Note that the input and output type are automatically inferred!\n# You do not need to specify them.\n# runnable = RunnableLambda(func).with_types( # <-- Not needed in this case\n#     input_schema=Foo,\n#     output_schema=int,\n# \nadd_routes(app, RunnableLambda(func), path=\"/foo\")\n\nPlayground Widgetsâ€‹\n\nThe playground allows you to define custom widgets for your runnable from the backend.\n\nA widget is specified at the field level and shipped as part of the JSON schema of the input type\nA widget must contain a key called type with the value being one of a well known list of widgets\nOther widget keys will be associated with values that describe paths in a JSON object\n\nGeneral schema:\n\ntype JsonPath = number | string | (number | string)[];\ntype NameSpacedPath = { title: string; path: JsonPath }; // Using title to mimick json schema, but can use namespace\ntype OneOfPath = { oneOf: JsonPath[] };\n\ntype Widget = {\n    type: string // Some well known type (e.g., base64file, chat etc.)\n    [key: string]: JsonPath | NameSpacedPath | OneOfPath;\n};\n\nFile Upload Widgetâ€‹\n\nAllows creation of a file upload input in the UI playground for files that are uploaded as base64 encoded strings. Here's the full example.\n\nSnippet:\n\ntry:\n    from pydantic.v1 import Field\nexcept ImportError:\n    from pydantic import Field\n\nfrom langserve import CustomUserType\n\n\n# ATTENTION: Inherit from CustomUserType instead of BaseModel otherwise\n#            the server will decode it into a dict instead of a pydantic model.\nclass FileProcessingRequest(CustomUserType):\n    \"\"\"Request including a base64 encoded file.\"\"\"\n\n    # The extra field is used to specify a widget for the playground UI.\n    file: str = Field(..., extra={\"widget\": {\"type\": \"base64file\"}})\n    num_chars: int = 100\n\n\n\nExample widget:\n\nEnabling / Disabling Endpoints (LangServe >=0.0.33)â€‹\n\nYou can enable / disable which endpoints are exposed. Use enabled_endpoints if you want to make sure to never get a new endpoint when upgrading langserve to a newer verison.\n\nEnable: The code below will only enable invoke, batch and the corresponding config_hash endpoint variants.\n\nadd_routes(app, chain, enabled_endpoints=[\"invoke\", \"batch\", \"config_hashes\"])\n\n\nDisable: The code below will disable the playground for the chain\n\nadd_routes(app, chain, disabled_endpoints=[\"playground\"]) \n\nPrevious\nToken counting\nNext\nLangSmith\nOverview\nFeatures\nLimitations\nHosted LangServe\nSecurity\nInstallation\nLangChain CLI ðŸ› ï¸\nExamples\nServer\nDocs\nClient\nEndpoints\nPlayground\nWidgets\nSharing\nLegacy Chains\nDeployment\nDeploy to Azure\nDeploy to GCP\nPydantic\nAdvanced\nHandling Authentication\nFiles\nCustom Input and Output Types\nCustom User Types\nPlayground Widgets\nEnabling / Disabling Endpoints (LangServe >=0.0.33)\nCommunity\nDiscord\nTwitter\nGitHub\nPython\nJS/TS\nMore\nHomepage\nBlog\nCopyright Â© 2023 LangChain, Inc."
}