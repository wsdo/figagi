{
	"title": "Retrieval-augmented generation (RAG) | ü¶úÔ∏èüîó Langchain",
	"url": "https://python.langchain.com/docs/use_cases/question_answering/",
	"html": "Skip to main content\nü¶úÔ∏èüîó LangChain\nDocs\nUse cases\nIntegrations\nGuides\nAPI\nMore\nü¶úÔ∏èüîó\nChat\nSearch\n‚åò\nK\nQA over structured data\nSQL\nRetrieval-augmented generation (RAG)\nRAG over code\nRAG with Agents\nText splitting by header\nRAG using local models\nInteracting with APIs\nChatbots\nExtraction\nSummarization\nTagging\nWeb scraping\nSynthetic data generation\nGraph querying\nRetrieval-augmented generation (RAG)\nRetrieval-augmented generation (RAG)\n\nOpen In Colab\n\nOverview‚Äã\nWhat is RAG?‚Äã\n\nRAG is a technique for augmenting LLM knowledge with additional, often private or real-time, data.\n\nLLMs can reason about wide-ranging topics, but their knowledge is limited to the public data up to a specific point in time that they were trained on. If you want to build AI applications that can reason about private data or data introduced after a model‚Äôs cutoff date, you need to augment the knowledge of the model with the specific information it needs. The process of bringing the appropriate information and inserting it into the model prompt is known as Retrieval Augmented Generation (RAG).\n\nWhat‚Äôs in this guide?‚Äã\n\nLangChain has a number of components specifically designed to help build RAG applications. To familiarize ourselves with these, we‚Äôll build a simple question-answering application over a text data source. Specifically, we‚Äôll build a QA bot over the LLM Powered Autonomous Agents blog post by Lilian Weng. Along the way we‚Äôll go over a typical QA architecture, discuss the relevant LangChain components, and highlight additional resources for more advanced QA techniques. We‚Äôll also see how LangSmith can help us trace and understand our application. LangSmith will become increasingly helpful as our application grows in complexity.\n\nNote Here we focus on RAG for unstructured data. Two RAG use cases which we cover elsewhere are: - QA over structured data (e.g., SQL) - QA over code (e.g., Python)\n\nArchitecture‚Äã\n\nA typical RAG application has two main components:\n\nIndexing: a pipeline for ingesting data from a source and indexing it. This usually happen offline.\n\nRetrieval and generation: the actual RAG chain, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\n\nThe most common full sequence from raw data to answer looks like:\n\nIndexing‚Äã\nLoad: First we need to load our data. We‚Äôll use DocumentLoaders for this.\nSplit: Text splitters break large Documents into smaller chunks. This is useful both for indexing data and for passing it in to a model, since large chunks are harder to search over and won‚Äôt in a model‚Äôs finite context window.\nStore: We need somewhere to store and index our splits, so that they can later be searched over. This is often done using a VectorStore and Embeddings model.\n\nRetrieval and generation‚Äã\nRetrieve: Given a user input, relevant splits are retrieved from storage using a Retriever.\nGenerate: A ChatModel / LLM produces an answer using a prompt that includes the question and the retrieved data\n\nSetup‚Äã\nDependencies‚Äã\n\nWe‚Äôll use an OpenAI chat model and embeddings and a Chroma vector store in this walkthrough, but everything shown here works with any ChatModel or LLM, Embeddings, and VectorStore or Retriever.\n\nWe‚Äôll use the following packages:\n\n!pip install -U langchain openai chromadb langchainhub bs4\n\n\nWe need to set environment variable OPENAI_API_KEY, which can be done directly or loaded from a .env file like so:\n\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\n# import dotenv\n\n# dotenv.load_dotenv()\n\nLangSmith‚Äã\n\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith.\n\nNote that LangSmith is not needed, but it is helpful. If you do want to use LangSmith, after you sign up at the link above, make sure to set your environment variables to start logging traces:\n\nos.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\nos.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass()\n\nQuickstart‚Äã\n\nSuppose we want to build a QA app over the LLM Powered Autonomous Agents blog post by Lilian Weng. We can create a simple pipeline for this in ~20 lines of code:\n\nimport bs4\nfrom langchain import hub\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.document_loaders import WebBaseLoader\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.schema import StrOutputParser\nfrom langchain.schema.runnable import RunnablePassthrough\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.vectorstores import Chroma\n\nloader = WebBaseLoader(\n    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n    bs_kwargs=dict(\n        parse_only=bs4.SoupStrainer(\n            class_=(\"post-content\", \"post-title\", \"post-header\")\n        )\n    ),\n)\ndocs = loader.load()\n\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\nsplits = text_splitter.split_documents(docs)\n\nvectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\nretriever = vectorstore.as_retriever()\n\nprompt = hub.pull(\"rlm/rag-prompt\")\nllm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n\n\ndef format_docs(docs):\n    return \"\\n\\n\".join(doc.page_content for doc in docs)\n\n\nrag_chain = (\n    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n    | prompt\n    | llm\n    | StrOutputParser()\n)\n\nrag_chain.invoke(\"What is Task Decomposition?\")\n\n'Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. It can be done through prompting techniques like Chain of Thought or Tree of Thoughts, or by using task-specific instructions or human inputs. Task decomposition helps agents plan ahead and manage complicated tasks more effectively.'\n\n# cleanup\nvectorstore.delete_collection()\n\n\nCheck out the LangSmith trace\n\nDetailed walkthrough‚Äã\n\nLet‚Äôs go through the above code step-by-step to really understand what‚Äôs going on.\n\nStep 1. Load‚Äã\n\nWe need to first load the blog post contents. We can use DocumentLoaders for this, which are objects that load in data from a source as Documents. A Document is an object with page_content (str) and metadata (dict) attributes.\n\nIn this case we‚Äôll use the WebBaseLoader, which uses urllib and BeautifulSoup to load and parse the passed in web urls, returning one Document per url. We can customize the html -> text parsing by passing in parameters to the BeautifulSoup parser via bs_kwargs (see BeautifulSoup docs). In this case only HTML tags with class ‚Äúpost-content‚Äù, ‚Äúpost-title‚Äù, or ‚Äúpost-header‚Äù are relevant, so we‚Äôll remove all others.\n\nfrom langchain.document_loaders import WebBaseLoader\n\nloader = WebBaseLoader(\n    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n    bs_kwargs={\n        \"parse_only\": bs4.SoupStrainer(\n            class_=(\"post-content\", \"post-title\", \"post-header\")\n        )\n    },\n)\ndocs = loader.load()\n\nlen(docs[0].page_content)\n\n42824\n\nprint(docs[0].page_content[:500])\n\n\n\n      LLM Powered Autonomous Agents\n    \nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n\n\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\nAgent System Overview#\nIn\n\nGo deeper‚Äã\n\nDocumentLoader: Object that load data from a source as Documents. - Docs: Further documentation on how to use DocumentLoaders. - Integrations: Find the relevant DocumentLoader integration (of the > 160 of them) for your use case.\n\nStep 2. Split‚Äã\n\nOur loaded document is over 42k characters long. This is too long to fit in the context window of many models. And even for those models that could fit the full post in their context window, empirically models struggle to find the relevant context in very long prompts.\n\nSo we‚Äôll split the Document into chunks for embedding and vector storage. This should help us retrieve only the most relevant bits of the blog post at run time.\n\nIn this case we‚Äôll split our documents into chunks of 1000 characters with 200 characters of overlap between chunks. The overlap helps mitigate the possibility of separating a statement from important context related to it. We use the RecursiveCharacterTextSplitter, which will (recursively) split the document using common separators (like new lines) until each chunk is the appropriate size.\n\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000, chunk_overlap=200, add_start_index=True\n)\nall_splits = text_splitter.split_documents(docs)\n\nlen(all_splits)\n\n66\n\nlen(all_splits[0].page_content)\n\n969\n\nall_splits[10].metadata\n\n{'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n 'start_index': 7056}\n\nGo deeper‚Äã\n\nDocumentSplitter: Object that splits a list of Documents into smaller chunks. Subclass of DocumentTransformers. - Explore Context-aware splitters, which keep the location (‚Äúcontext‚Äù) of each split in the original Document: - Markdown files - Code (py or js) - Scientific papers\n\nDocumentTransformer: Object that performs a transformation on a list of Documents. - Docs: Further documentation on how to use DocumentTransformers - Integrations\n\nStep 3. Store‚Äã\n\nNow that we‚Äôve got 66 text chunks in memory, we need to store and index them so that we can search them later in our RAG app. The most common way to do this is to embed the contents of each document split and upload those embeddings to a vector store.\n\nThen, when we want to search over our splits, we take the search query, embed it as well, and perform some sort of ‚Äúsimilarity‚Äù search to identify the stored splits with the most similar embeddings to our query embedding. The simplest similarity measure is cosine similarity ‚Äî¬†we measure the cosine of the angle between each pair of embeddings (which are just very high dimensional vectors).\n\nWe can embed and store all of our document splits in a single command using the Chroma vector store and OpenAIEmbeddings model.\n\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.vectorstores import Chroma\n\nvectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())\n\nGo deeper‚Äã\n\nEmbeddings: Wrapper around a text embedding model, used for converting text to embeddings. - Docs: Further documentation on the interface. - Integrations: Browse the > 30 text embedding integrations\n\nVectorStore: Wrapper around a vector database, used for storing and querying embeddings. - Docs: Further documentation on the interface. - Integrations: Browse the > 40 VectorStore integrations.\n\nThis completes the Indexing portion of the pipeline. At this point we have an query-able vector store containing the chunked contents of our blog post. Given a user question, we should ideally be able to return the snippets of the blog post that answer the question:\n\nStep 4. Retrieve‚Äã\n\nNow let‚Äôs write the actual application logic. We want to create a simple application that let‚Äôs the user ask a question, searches for documents relevant to that question, passes the retrieved documents and initial question to a model, and finally returns an answer.\n\nLangChain defines a Retriever interface which wraps an index that can return relevant documents given a string query. All retrievers implement a common method get_relevant_documents() (and its asynchronous variant aget_relevant_documents()).\n\nThe most common type of Retriever is the VectorStoreRetriever, which uses the similarity search capabilities of a vector store to facillitate retrieval. Any VectorStore can easily be turned into a Retriever:\n\nretriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})\n\nretrieved_docs = retriever.get_relevant_documents(\n    \"What are the approaches to Task Decomposition?\"\n)\n\nlen(retrieved_docs)\n\n6\n\nprint(retrieved_docs[0].page_content)\n\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n\nGo deeper‚Äã\n\nVector stores are commonly used for retrieval, but there are plenty of other ways to do retrieval.\n\nRetriever: An object that returns Documents given a text query - Docs: Further documentation on the interface and built-in retrieval techniques. Some of which include: - MultiQueryRetriever generates variants of the input question to improve retrieval hit rate. - MultiVectorRetriever (diagram below) instead generates variants of the embeddings, also in order to improve retrieval hit rate. - Max marginal relevance selects for relevance and diversity among the retrieved documents to avoid passing in duplicate context. - Documents can be filtered during vector store retrieval using metadata filters. - Integrations: Integrations with retrieval services.\n\nStep 5. Generate‚Äã\n\nLet‚Äôs put it all together into a chain that takes a question, retrieves relevant documents, constructs a prompt, passes that to a model, and parses the output.\n\nWe‚Äôll use the gpt-3.5-turbo OpenAI chat model, but any LangChain LLM or ChatModel could be substituted in.\n\nfrom langchain.chat_models import ChatOpenAI\n\nllm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n\n\nWe‚Äôll use a prompt for RAG that is checked into the LangChain prompt hub (here).\n\nfrom langchain import hub\n\nprompt = hub.pull(\"rlm/rag-prompt\")\n\nprint(\n    prompt.invoke(\n        {\"context\": \"filler context\", \"question\": \"filler question\"}\n    ).to_string()\n)\n\nHuman: You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\nQuestion: filler question \nContext: filler context \nAnswer:\n\n\nWe‚Äôll use the LCEL Runnable protocol to define the chain, allowing us to - pipe together components and functions in a transparent way - automatically trace our chain in LangSmith - get streaming, async, and batched calling out of the box\n\nfrom langchain.schema import StrOutputParser\nfrom langchain.schema.runnable import RunnablePassthrough\n\n\ndef format_docs(docs):\n    return \"\\n\\n\".join(doc.page_content for doc in docs)\n\n\nrag_chain = (\n    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n    | prompt\n    | llm\n    | StrOutputParser()\n)\n\nfor chunk in rag_chain.stream(\"What is Task Decomposition?\"):\n    print(chunk, end=\"\", flush=True)\n\nTask decomposition is a technique used to break down complex tasks into smaller and simpler steps. It can be done through methods like Chain of Thought (CoT) or Tree of Thoughts, which involve dividing the task into manageable subtasks and exploring multiple reasoning possibilities at each step. Task decomposition can be performed by AI models with prompting, task-specific instructions, or human inputs.\n\n\nCheck out the LangSmith trace\n\nGo deeper‚Äã\nChoosing LLMs‚Äã\n\nChatModel: An LLM-backed chat model wrapper. Takes in a sequence of messages and returns a message. - Docs - Integrations: Explore over 25 ChatModel integrations.\n\nLLM: A text-in-text-out LLM. Takes in a string and returns a string. - Docs - Integrations: Explore over 75 LLM integrations.\n\nSee a guide on RAG with locally-running models here.\n\nCustomizing the prompt‚Äã\n\nAs shown above, we can load prompts (e.g., this RAG prompt) from the prompt hub. The prompt can also be easily customized:\n\nfrom langchain.prompts import PromptTemplate\n\ntemplate = \"\"\"Use the following pieces of context to answer the question at the end.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\nUse three sentences maximum and keep the answer as concise as possible.\nAlways say \"thanks for asking!\" at the end of the answer.\n{context}\nQuestion: {question}\nHelpful Answer:\"\"\"\nrag_prompt_custom = PromptTemplate.from_template(template)\n\nrag_chain = (\n    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n    | rag_prompt_custom\n    | llm\n    | StrOutputParser()\n)\n\nrag_chain.invoke(\"What is Task Decomposition?\")\n\n'Task decomposition is the process of breaking down a complex task into smaller and simpler steps. It can be done through techniques like Chain of Thought (CoT) or Tree of Thoughts, which involve dividing the problem into multiple thought steps and generating multiple thoughts per step. Task decomposition helps in enhancing model performance and understanding the thinking process of the model. Thanks for asking!'\n\n\nCheck out the LangSmith trace\n\nAdding sources‚Äã\n\nWith LCEL it‚Äôs easy to return the retrieved documents or certain source metadata from the documents:\n\nfrom operator import itemgetter\n\nfrom langchain.schema.runnable import RunnableParallel\n\nrag_chain_from_docs = (\n    {\n        \"context\": lambda input: format_docs(input[\"documents\"]),\n        \"question\": itemgetter(\"question\"),\n    }\n    | rag_prompt_custom\n    | llm\n    | StrOutputParser()\n)\nrag_chain_with_source = RunnableParallel(\n    {\"documents\": retriever, \"question\": RunnablePassthrough()}\n) | {\n    \"documents\": lambda input: [doc.metadata for doc in input[\"documents\"]],\n    \"answer\": rag_chain_from_docs,\n}\n\nrag_chain_with_source.invoke(\"What is Task Decomposition\")\n\n{'documents': [{'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n   'start_index': 1585},\n  {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n   'start_index': 2192},\n  {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n   'start_index': 17804},\n  {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n   'start_index': 17414},\n  {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n   'start_index': 29630},\n  {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n   'start_index': 19373}],\n 'answer': 'Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. It involves transforming big tasks into multiple manageable tasks, allowing for a more systematic and organized approach to problem-solving. Thanks for asking!'}\n\n\nCheck out the LangSmith trace\n\nAdding memory‚Äã\n\nSuppose we want to create a stateful application that remembers past user inputs. There are two main things we need to do to support this. 1. Add a messages placeholder to our chain which allows us to pass in historical messages 2. Add a chain that takes the latest user query and reformulates it in the context of the chat history into a standalone question that can be passed to our retriever.\n\nLet‚Äôs start with 2. We can build a ‚Äúcondense question‚Äù chain that looks something like this:\n\nfrom langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n\ncondense_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\nwhich might reference the chat history, formulate a standalone question \\\nwhich can be understood without the chat history. Do NOT answer the question, \\\njust reformulate it if needed and otherwise return it as is.\"\"\"\ncondense_q_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", condense_q_system_prompt),\n        MessagesPlaceholder(variable_name=\"chat_history\"),\n        (\"human\", \"{question}\"),\n    ]\n)\ncondense_q_chain = condense_q_prompt | llm | StrOutputParser()\n\nfrom langchain.schema.messages import AIMessage, HumanMessage\n\ncondense_q_chain.invoke(\n    {\n        \"chat_history\": [\n            HumanMessage(content=\"What does LLM stand for?\"),\n            AIMessage(content=\"Large language model\"),\n        ],\n        \"question\": \"What is meant by large\",\n    }\n)\n\n'What is the definition of \"large\" in the context of a language model?'\n\ncondense_q_chain.invoke(\n    {\n        \"chat_history\": [\n            HumanMessage(content=\"What does LLM stand for?\"),\n            AIMessage(content=\"Large language model\"),\n        ],\n        \"question\": \"How do transformers work\",\n    }\n)\n\n'How do transformer models function?'\n\n\nAnd now we can build our full QA chain. Notice we add some routing functionality to only run the ‚Äúcondense question chain‚Äù when our chat history isn‚Äôt empty.\n\nqa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\nUse the following pieces of retrieved context to answer the question. \\\nIf you don't know the answer, just say that you don't know. \\\nUse three sentences maximum and keep the answer concise.\\\n\n{context}\"\"\"\nqa_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", qa_system_prompt),\n        MessagesPlaceholder(variable_name=\"chat_history\"),\n        (\"human\", \"{question}\"),\n    ]\n)\n\n\ndef condense_question(input: dict):\n    if input.get(\"chat_history\"):\n        return condense_q_chain\n    else:\n        return input[\"question\"]\n\n\nrag_chain = (\n    RunnablePassthrough.assign(context=condense_question | retriever | format_docs)\n    | qa_prompt\n    | llm\n)\n\nchat_history = []\n\nquestion = \"What is Task Decomposition?\"\nai_msg = rag_chain.invoke({\"question\": question, \"chat_history\": chat_history})\nchat_history.extend([HumanMessage(content=question), ai_msg])\n\nsecond_question = \"What are common ways of doing it?\"\nrag_chain.invoke({\"question\": second_question, \"chat_history\": chat_history})\n\nAIMessage(content='Common ways of task decomposition include:\\n\\n1. Using Chain of Thought (CoT): CoT is a prompting technique that instructs the model to \"think step by step\" and decompose complex tasks into smaller and simpler steps. It utilizes more test-time computation and sheds light on the model\\'s thinking process.\\n\\n2. Prompting with LLM: Language Model (LLM) can be used to prompt the model with simple instructions like \"Steps for XYZ\" or \"What are the subgoals for achieving XYZ?\" This allows the model to generate a sequence of subtasks or thought steps.\\n\\n3. Task-specific instructions: For certain tasks, task-specific instructions can be provided to guide the model in decomposing the task. For example, for writing a novel, the instruction \"Write a story outline\" can be given to break down the task into manageable steps.\\n\\n4. Human inputs: In some cases, human inputs can be used to assist in task decomposition. Humans can provide their expertise and knowledge to identify and break down complex tasks into smaller subtasks.')\n\n\nCheck out the LangSmith trace\n\nHere we‚Äôve gone over how to add chain logic for incorporating historical outputs. But how do we actually store and retrieve historical outputs for different sessions? For that check out the LCEL How to add message history (memory) page.\n\nNext steps‚Äã\n\nThat‚Äôs a lot of content we‚Äôve covered in a short amount of time. There‚Äôs plenty of nuances, features, integrations, etc to explore in each of the above sections. Aside from the sources mentioned above, good next steps include:\n\nReading up on more advanced retrieval techniques in the Retrievers section.\nLearning about the LangChain Indexing API, which helps repeatedly sync data sources and vector stores without redundant computation or storage.\nExploring RAG LangChain Templates, which are reference applications that can easily be deployed with LangServe.\nLearning about evaluating RAG applications with LangSmith.\nPrevious\nSQL\nNext\nRAG over code\nOverview\nWhat is RAG?\nWhat‚Äôs in this guide?\nArchitecture\nSetup\nDependencies\nLangSmith\nQuickstart\nDetailed walkthrough\nStep 1. Load\nGo deeper\nStep 2. Split\nGo deeper\nStep 3. Store\nGo deeper\nStep 4. Retrieve\nGo deeper\nStep 5. Generate\nGo deeper\nAdding sources\nAdding memory\nNext steps\nCommunity\nDiscord\nTwitter\nGitHub\nPython\nJS/TS\nMore\nHomepage\nBlog\nCopyright ¬© 2023 LangChain, Inc."
}