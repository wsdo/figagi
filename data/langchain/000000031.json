{
	"title": "Extraction | ü¶úÔ∏èüîó Langchain",
	"url": "https://python.langchain.com/docs/use_cases/extraction",
	"html": "Skip to main content\nü¶úÔ∏èüîó LangChain\nDocs\nUse cases\nIntegrations\nGuides\nAPI\nMore\nü¶úÔ∏èüîó\nChat\nSearch\n‚åò\nK\nQA over structured data\nSQL\nRetrieval-augmented generation (RAG)\nInteracting with APIs\nChatbots\nExtraction\nSummarization\nTagging\nWeb scraping\nSynthetic data generation\nGraph querying\nExtraction\nExtraction\n\nOpen In Collab\n\nUse case‚Äã\n\nGetting structured output from raw LLM generations is hard.\n\nFor example, suppose you need the model output formatted with a specific schema for:\n\nExtracting a structured row to insert into a database\nExtracting API parameters\nExtracting different parts of a user query (e.g., for semantic vs keyword search)\n\nOverview‚Äã\n\nThere are two primary approaches for this:\n\nFunctions: Some LLMs can call functions to extract arbitrary entities from LLM responses.\n\nParsing: Output parsers are classes that structure LLM responses.\n\nOnly some LLMs support functions (e.g., OpenAI), and they are more general than parsers.\n\nParsers extract precisely what is enumerated in a provided schema (e.g., specific attributes of a person).\n\nFunctions can infer things beyond of a provided schema (e.g., attributes about a person that you did not ask for).\n\nQuickstart‚Äã\n\nOpenAI functions are one way to get started with extraction.\n\nDefine a schema that specifies the properties we want to extract from the LLM output.\n\nThen, we can use create_extraction_chain to extract our desired schema using an OpenAI function call.\n\npip install langchain openai \n\n# Set env var OPENAI_API_KEY or load from a .env file:\n# import dotenv\n# dotenv.load_dotenv()\n\nfrom langchain.chains import create_extraction_chain\nfrom langchain.chat_models import ChatOpenAI\n\n# Schema\nschema = {\n    \"properties\": {\n        \"name\": {\"type\": \"string\"},\n        \"height\": {\"type\": \"integer\"},\n        \"hair_color\": {\"type\": \"string\"},\n    },\n    \"required\": [\"name\", \"height\"],\n}\n\n# Input\ninp = \"\"\"Alex is 5 feet tall. Claudia is 1 feet taller Alex and jumps higher than him. Claudia is a brunette and Alex is blonde.\"\"\"\n\n# Run chain\nllm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\")\nchain = create_extraction_chain(schema, llm)\nchain.run(inp)\n\n[{'name': 'Alex', 'height': 5, 'hair_color': 'blonde'},\n {'name': 'Claudia', 'height': 6, 'hair_color': 'brunette'}]\n\nOption 1: OpenAI functions‚Äã\nLooking under the hood‚Äã\n\nLet‚Äôs dig into what is happening when we call create_extraction_chain.\n\nThe LangSmith trace shows that we call the function information_extraction on the input string, inp.\n\nThis information_extraction function is defined here and returns a dict.\n\nWe can see the dict in the model output:\n\n {\n      \"info\": [\n        {\n          \"name\": \"Alex\",\n          \"height\": 5,\n          \"hair_color\": \"blonde\"\n        },\n        {\n          \"name\": \"Claudia\",\n          \"height\": 6,\n          \"hair_color\": \"brunette\"\n        }\n      ]\n    }\n\n\nThe create_extraction_chain then parses the raw LLM output for us using JsonKeyOutputFunctionsParser.\n\nThis results in the list of JSON objects returned by the chain above:\n\n[{'name': 'Alex', 'height': 5, 'hair_color': 'blonde'},\n {'name': 'Claudia', 'height': 6, 'hair_color': 'brunette'}]\n\nMultiple entity types‚Äã\n\nWe can extend this further.\n\nLet‚Äôs say we want to differentiate between dogs and people.\n\nWe can add person_ and dog_ prefixes for each property\n\nschema = {\n    \"properties\": {\n        \"person_name\": {\"type\": \"string\"},\n        \"person_height\": {\"type\": \"integer\"},\n        \"person_hair_color\": {\"type\": \"string\"},\n        \"dog_name\": {\"type\": \"string\"},\n        \"dog_breed\": {\"type\": \"string\"},\n    },\n    \"required\": [\"person_name\", \"person_height\"],\n}\n\nchain = create_extraction_chain(schema, llm)\n\ninp = \"\"\"Alex is 5 feet tall. Claudia is 1 feet taller Alex and jumps higher than him. Claudia is a brunette and Alex is blonde.\nAlex's dog Frosty is a labrador and likes to play hide and seek.\"\"\"\n\nchain.run(inp)\n\n[{'person_name': 'Alex',\n  'person_height': 5,\n  'person_hair_color': 'blonde',\n  'dog_name': 'Frosty',\n  'dog_breed': 'labrador'},\n {'person_name': 'Claudia',\n  'person_height': 6,\n  'person_hair_color': 'brunette'}]\n\nUnrelated entities‚Äã\n\nIf we use required: [], we allow the model to return only person attributes or only dog attributes for a single entity (person or dog).\n\nschema = {\n    \"properties\": {\n        \"person_name\": {\"type\": \"string\"},\n        \"person_height\": {\"type\": \"integer\"},\n        \"person_hair_color\": {\"type\": \"string\"},\n        \"dog_name\": {\"type\": \"string\"},\n        \"dog_breed\": {\"type\": \"string\"},\n    },\n    \"required\": [],\n}\n\nchain = create_extraction_chain(schema, llm)\n\ninp = \"\"\"Alex is 5 feet tall. Claudia is 1 feet taller Alex and jumps higher than him. Claudia is a brunette and Alex is blonde.\nWillow is a German Shepherd that likes to play with other dogs and can always be found playing with Milo, a border collie that lives close by.\"\"\"\n\nchain.run(inp)\n\n[{'person_name': 'Alex', 'person_height': 5, 'person_hair_color': 'blonde'},\n {'person_name': 'Claudia',\n  'person_height': 6,\n  'person_hair_color': 'brunette'},\n {'dog_name': 'Willow', 'dog_breed': 'German Shepherd'},\n {'dog_name': 'Milo', 'dog_breed': 'border collie'}]\n\nExtra information‚Äã\n\nThe power of functions (relative to using parsers alone) lies in the ability to perform semantic extraction.\n\nIn particular, we can ask for things that are not explicitly enumerated in the schema.\n\nSuppose we want unspecified additional information about dogs.\n\nWe can use add a placeholder for unstructured extraction, dog_extra_info.\n\nschema = {\n    \"properties\": {\n        \"person_name\": {\"type\": \"string\"},\n        \"person_height\": {\"type\": \"integer\"},\n        \"person_hair_color\": {\"type\": \"string\"},\n        \"dog_name\": {\"type\": \"string\"},\n        \"dog_breed\": {\"type\": \"string\"},\n        \"dog_extra_info\": {\"type\": \"string\"},\n    },\n}\n\nchain = create_extraction_chain(schema, llm)\nchain.run(inp)\n\n[{'person_name': 'Alex', 'person_height': 5, 'person_hair_color': 'blonde'},\n {'person_name': 'Claudia',\n  'person_height': 6,\n  'person_hair_color': 'brunette'},\n {'dog_name': 'Willow',\n  'dog_breed': 'German Shepherd',\n  'dog_extra_info': 'likes to play with other dogs'},\n {'dog_name': 'Milo',\n  'dog_breed': 'border collie',\n  'dog_extra_info': 'lives close by'}]\n\n\nThis gives us additional information about the dogs.\n\nPydantic‚Äã\n\nPydantic is a data validation and settings management library for Python.\n\nIt allows you to create data classes with attributes that are automatically validated when you instantiate an object.\n\nLets define a class with attributes annotated with types.\n\nfrom typing import Optional\n\nfrom langchain.chains import create_extraction_chain_pydantic\nfrom langchain.pydantic_v1 import BaseModel\n\n\n# Pydantic data class\nclass Properties(BaseModel):\n    person_name: str\n    person_height: int\n    person_hair_color: str\n    dog_breed: Optional[str]\n    dog_name: Optional[str]\n\n\n# Extraction\nchain = create_extraction_chain_pydantic(pydantic_schema=Properties, llm=llm)\n\n# Run\ninp = \"\"\"Alex is 5 feet tall. Claudia is 1 feet taller Alex and jumps higher than him. Claudia is a brunette and Alex is blonde.\"\"\"\nchain.run(inp)\n\n[Properties(person_name='Alex', person_height=5, person_hair_color='blonde', dog_breed=None, dog_name=None),\n Properties(person_name='Claudia', person_height=6, person_hair_color='brunette', dog_breed=None, dog_name=None)]\n\n\nAs we can see from the trace, we use the function information_extraction, as above, with the Pydantic schema.\n\nOption 2: Parsing‚Äã\n\nOutput parsers are classes that help structure language model responses.\n\nAs shown above, they are used to parse the output of the OpenAI function calls in create_extraction_chain.\n\nBut, they can be used independent of functions.\n\nPydantic‚Äã\n\nJust as a above, let‚Äôs parse a generation based on a Pydantic data class.\n\nfrom typing import Optional, Sequence\n\nfrom langchain.llms import OpenAI\nfrom langchain.output_parsers import PydanticOutputParser\nfrom langchain.prompts import (\n    PromptTemplate,\n)\nfrom pydantic import BaseModel, Field, validator\n\n\nclass Person(BaseModel):\n    person_name: str\n    person_height: int\n    person_hair_color: str\n    dog_breed: Optional[str]\n    dog_name: Optional[str]\n\n\nclass People(BaseModel):\n    \"\"\"Identifying information about all people in a text.\"\"\"\n\n    people: Sequence[Person]\n\n\n# Run\nquery = \"\"\"Alex is 5 feet tall. Claudia is 1 feet taller Alex and jumps higher than him. Claudia is a brunette and Alex is blonde.\"\"\"\n\n# Set up a parser + inject instructions into the prompt template.\nparser = PydanticOutputParser(pydantic_object=People)\n\n# Prompt\nprompt = PromptTemplate(\n    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n    input_variables=[\"query\"],\n    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n)\n\n# Run\n_input = prompt.format_prompt(query=query)\nmodel = OpenAI(temperature=0)\noutput = model(_input.to_string())\nparser.parse(output)\n\nPeople(people=[Person(person_name='Alex', person_height=5, person_hair_color='blonde', dog_breed=None, dog_name=None), Person(person_name='Claudia', person_height=6, person_hair_color='brunette', dog_breed=None, dog_name=None)])\n\n\nWe can see from the LangSmith trace that we get the same output as above.\n\nWe can see that we provide a two-shot prompt in order to instruct the LLM to output in our desired format.\n\nAnd, we need to do a bit more work:\n\nDefine a class that holds multiple instances of Person\nExplicitly parse the output of the LLM to the Pydantic class\n\nWe can see this for other cases, too.\n\nfrom langchain.llms import OpenAI\nfrom langchain.output_parsers import PydanticOutputParser\nfrom langchain.prompts import (\n    PromptTemplate,\n)\nfrom pydantic import BaseModel, Field, validator\n\n\n# Define your desired data structure.\nclass Joke(BaseModel):\n    setup: str = Field(description=\"question to set up a joke\")\n    punchline: str = Field(description=\"answer to resolve the joke\")\n\n    # You can add custom validation logic easily with Pydantic.\n    @validator(\"setup\")\n    def question_ends_with_question_mark(cls, field):\n        if field[-1] != \"?\":\n            raise ValueError(\"Badly formed question!\")\n        return field\n\n\n# And a query intended to prompt a language model to populate the data structure.\njoke_query = \"Tell me a joke.\"\n\n# Set up a parser + inject instructions into the prompt template.\nparser = PydanticOutputParser(pydantic_object=Joke)\n\n# Prompt\nprompt = PromptTemplate(\n    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n    input_variables=[\"query\"],\n    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n)\n\n# Run\n_input = prompt.format_prompt(query=joke_query)\nmodel = OpenAI(temperature=0)\noutput = model(_input.to_string())\nparser.parse(output)\n\nJoke(setup='Why did the chicken cross the road?', punchline='To get to the other side!')\n\n\nAs we can see, we get an output of the Joke class, which respects our originally desired schema: ‚Äòsetup‚Äô and ‚Äòpunchline‚Äô.\n\nWe can look at the LangSmith trace to see exactly what is going on under the hood.\n\nGoing deeper‚Äã\nThe output parser documentation includes various parser examples for specific types (e.g., lists, datetime, enum, etc).\nThe experimental Anthropic function calling support provides similar functionality to Anthropic chat models.\nLlamaCPP natively supports constrained decoding using custom grammars, making it easy to output structured content using local LLMs\nJSONFormer offers another way for structured decoding of a subset of the JSON Schema.\nKor is another library for extraction where schema and examples can be provided to the LLM.\nPrevious\nChatbots\nNext\nSummarization\nUse case\nOverview\nQuickstart\nOption 1: OpenAI functions\nLooking under the hood\nMultiple entity types\nUnrelated entities\nExtra information\nPydantic\nOption 2: Parsing\nPydantic\nGoing deeper\nCommunity\nDiscord\nTwitter\nGitHub\nPython\nJS/TS\nMore\nHomepage\nBlog\nCopyright ¬© 2023 LangChain, Inc."
}