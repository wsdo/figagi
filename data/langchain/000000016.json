{
	"title": "Get started | 🦜️🔗 Langchain",
	"url": "https://python.langchain.com/docs/expression_language/get_started",
	"html": "Skip to main content\n🦜️🔗 LangChain\nDocs\nUse cases\nIntegrations\nGuides\nAPI\nMore\n🦜️🔗\nChat\nSearch\n⌘\nK\nGet started\nIntroduction\nInstallation\nQuickstart\nSecurity\nLangChain Expression Language\nGet started\nWhy use LCEL\nInterface\nHow to\nCookbook\nModules\nModel I/O\nRetrieval\nAgents\nMore\nLangServe\nLangSmith\nLangChain Expression LanguageGet started\nGet started\n\nLCEL makes it easy to build complex chains from basic components, and supports out of the box functionality such as streaming, parallelism, and logging.\n\nBasic example: prompt + model + output parser​\n\nThe most basic and common use case is chaining a prompt template and a model together. To see how this works, let’s create a chain that takes a topic and generates a joke:\n\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain.schema.output_parser import StrOutputParser\n\nprompt = ChatPromptTemplate.from_template(\"tell me a short joke about {topic}\")\nmodel = ChatOpenAI()\noutput_parser = StrOutputParser()\n\nchain = prompt | model | output_parser\n\nchain.invoke({\"topic\": \"ice cream\"})\n\n\"Why did the ice cream go to therapy?\\n\\nBecause it had too many toppings and couldn't find its cone-fidence!\"\n\n\nNotice this line of this code, where we piece together then different components into a single chain using LCEL:\n\nchain = prompt | model | output_parser\n\n\nThe | symbol is similar to a unix pipe operator, which chains together the different components feeds the output from one component as input into the next component.\n\nIn this chain the user input is passed to the prompt template, then the prompt template output is passed to the model, then the model output is passed to the output parser. Let’s take a look at each component individually to really understand what’s going on.\n\n1. Prompt​\n\nprompt is a BasePromptTemplate, which means it takes in a dictionary of template variables and produces a PromptValue. A PromptValue is a wrapper around a completed prompt that can be passed to either an LLM (which takes a string as input) or ChatModel (which takes a sequence of messages as input). It can work with either language model type because it defines logic both for producing BaseMessages and for producing a string.\n\nprompt_value = prompt.invoke({\"topic\": \"ice cream\"})\nprompt_value\n\nChatPromptValue(messages=[HumanMessage(content='tell me a short joke about ice cream')])\n\nprompt_value.to_messages()\n\n[HumanMessage(content='tell me a short joke about ice cream')]\n\nprompt_value.to_string()\n\n'Human: tell me a short joke about ice cream'\n\n2. Model​\n\nThe PromptValue is then passed to model. In this case our model is a ChatModel, meaning it will output a BaseMessage.\n\nmessage = model.invoke(prompt_value)\nmessage\n\nAIMessage(content=\"Why did the ice cream go to therapy? \\n\\nBecause it had too many toppings and couldn't find its cone-fidence!\")\n\n\nIf our model was an LLM, it would output a string.\n\nfrom langchain.llms import OpenAI\n\nllm = OpenAI(model=\"gpt-3.5-turbo-instruct\")\nllm.invoke(prompt_value)\n\n'\\n\\nRobot: Why did the ice cream go to therapy? Because it had a rocky road.'\n\n3. Output parser​\n\nAnd lastly we pass our model output to the output_parser, which is a BaseOutputParser meaning it takes either a string or a BaseMessage as input. The StrOutputParser specifically simple converts any input into a string.\n\noutput_parser.invoke(message)\n\n\"Why did the ice cream go to therapy? \\n\\nBecause it had too many toppings and couldn't find its cone-fidence!\"\n\n4. Entire Pipeline​\n\nTo follow the steps along:\n\nWe pass in user input on the desired topic as {\"topic\": \"ice cream\"}\nThe prompt component takes the user input, which is then used to construct a PromptValue after using the topic to construct the prompt.\nThe model component takes the generated prompt, and passes into the OpenAI LLM model for evaluation. The generated output from the model is a ChatMessage object.\nFinally, the output_parser component takes in a ChatMessage, and transforms this into a Python string, which is returned from the invoke method.\nDict\nPromptValue\nChatMessage\nString\nInput: topic=ice cream\nPromptTemplate\nChatModel\nStrOutputParser\nResult\n\nNote that if you’re curious about the output of any components, you can always test out a smaller version of the chain such as prompt or prompt | model to see the intermediate results:\n\ninput = {\"topic\": \"ice cream\"}\n\nprompt.invoke(input)\n# > ChatPromptValue(messages=[HumanMessage(content='tell me a short joke about ice cream')])\n\n(prompt | model).invoke(input)\n# > AIMessage(content=\"Why did the ice cream go to therapy?\\nBecause it had too many toppings and couldn't cone-trol itself!\")\n\nRAG Search Example​\n\nFor our next example, we want to run a retrieval-augmented generation chain to add some context when responding to questions.\n\n# Requires:\n# pip install langchain docarray\n\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain.schema.output_parser import StrOutputParser\nfrom langchain.schema.runnable import RunnableParallel, RunnablePassthrough\nfrom langchain.vectorstores import DocArrayInMemorySearch\n\nvectorstore = DocArrayInMemorySearch.from_texts(\n    [\"harrison worked at kensho\", \"bears like to eat honey\"],\n    embedding=OpenAIEmbeddings(),\n)\nretriever = vectorstore.as_retriever()\n\ntemplate = \"\"\"Answer the question based only on the following context:\n{context}\n\nQuestion: {question}\n\"\"\"\nprompt = ChatPromptTemplate.from_template(template)\nmodel = ChatOpenAI()\noutput_parser = StrOutputParser()\n\nsetup_and_retrieval = RunnableParallel(\n    {\"context\": retriever, \"question\": RunnablePassthrough()}\n)\nchain = setup_and_retrieval | prompt | model | output_parser\n\nchain.invoke(\"where did harrison work?\")\n\n\nIn this case, the composed chain is:\n\nchain = setup_and_retrieval | prompt | model | output_parser\n\n\nTo explain this, we first can see that the prompt template above takes in context and question as values to be substituted in the prompt. Before building the prompt template, we want to retrieve relevant documents to the search and include them as part of the context.\n\nAs a preliminary step, we’ve setup the retriever using an in memory store, which can retrieve documents based on a query. This is a runnable component as well that can be chained together with other components, but you can also try to run it separately:\n\nretriever.invoke(\"where did harrison work?\")\n\n\nWe then use the RunnableParallel to prepare the expected inputs into the prompt by using the entries for the retrieved documents as well as the original user question, using the retriever for document search, and RunnablePassthrough to pass the user’s question:\n\nsetup_and_retrieval = RunnableParallel(\n    {\"context\": retriever, \"question\": RunnablePassthrough()}\n)\n\n\nTo review, the complete chain is:\n\nsetup_and_retrieval = RunnableParallel(\n    {\"context\": retriever, \"question\": RunnablePassthrough()}\n)\nchain = setup_and_retrieval | prompt | model | output_parser\n\n\nWith the flow being:\n\nThe first steps create a RunnableParallel object with two entries. The first entry, context will include the document results fetched by the retriever. The second entry, question will contain the user’s original question. To pass on the question, we use RunnablePassthrough to copy this entry.\nFeed the dictionary from the step above to the prompt component. It then takes the user input which is question as well as the retrieved document which is context to construct a prompt and output a PromptValue.\nThe model component takes the generated prompt, and passes into the OpenAI LLM model for evaluation. The generated output from the model is a ChatMessage object.\nFinally, the output_parser component takes in a ChatMessage, and transforms this into a Python string, which is returned from the invoke method.\nQuestion\nQuestion\ncontext=retrieved docs\nquestion=Question\nPromptValue\nChatMessage\nString\nQuestion\nRunnableParallel\nRetriever\nRunnablePassThrough\nPromptTemplate\nChatModel\nStrOutputParser\nResult\nNext steps​\n\nWe recommend reading our Why use LCEL section next to see a side-by-side comparison of the code needed to produce common functionality with and without LCEL.\n\nPrevious\nLangChain Expression Language (LCEL)\nNext\nWhy use LCEL\nBasic example: prompt + model + output parser\n1. Prompt\n2. Model\n3. Output parser\n4. Entire Pipeline\nRAG Search Example\nNext steps\nCommunity\nDiscord\nTwitter\nGitHub\nPython\nJS/TS\nMore\nHomepage\nBlog\nCopyright © 2023 LangChain, Inc."
}