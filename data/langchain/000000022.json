{
	"title": "Model I/O | ğŸ¦œï¸ğŸ”— Langchain",
	"url": "https://python.langchain.com/docs/modules/model_io/",
	"html": "Skip to main content\nğŸ¦œï¸ğŸ”— LangChain\nDocs\nUse cases\nIntegrations\nGuides\nAPI\nMore\nğŸ¦œï¸ğŸ”—\nChat\nSearch\nâŒ˜\nK\nGet started\nIntroduction\nInstallation\nQuickstart\nSecurity\nLangChain Expression Language\nGet started\nWhy use LCEL\nInterface\nHow to\nCookbook\nModules\nModel I/O\nPrompts\nChat models\nLLMs\nOutput parsers\nRetrieval\nAgents\nMore\nLangServe\nLangSmith\nModulesModel I/O\nModel I/O\n\nThe core element of any language model application is...the model. LangChain gives you the building blocks to interface with any language model.\n\nPrompts: Templatize, dynamically select, and manage model inputs\nChat models: Models that are backed by a language model but take a list of Chat Messages as input and return a Chat Message\nLLMs: Models that take a text string as input and return a text string\nOutput parsers: Extract information from model outputs\n\nLLMs vs Chat modelsâ€‹\n\nLLMs and chat models are subtly but importantly different. LLMs in LangChain refer to pure text completion models. The APIs they wrap take a string prompt as input and output a string completion. OpenAI's GPT-3 is implemented as an LLM. Chat models are often backed by LLMs but tuned specifically for having conversations. And, crucially, their provider APIs use a different interface than pure text completion models. Instead of a single string, they take a list of chat messages as input. Usually these messages are labeled with the speaker (usually one of \"System\", \"AI\", and \"Human\"). And they return an AI chat message as output. GPT-4 and Anthropic's Claude-2 are both implemented as chat models.\n\nPrevious\nModules\nNext\nModel I/O\nLLMs vs Chat models\nCommunity\nDiscord\nTwitter\nGitHub\nPython\nJS/TS\nMore\nHomepage\nBlog\nCopyright Â© 2023 LangChain, Inc."
}