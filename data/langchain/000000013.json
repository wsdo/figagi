{
	"title": "Quickstart | ðŸ¦œï¸ðŸ”— Langchain",
	"url": "https://python.langchain.com/docs/get_started/quickstart",
	"html": "Skip to main content\nðŸ¦œï¸ðŸ”— LangChain\nDocs\nUse cases\nIntegrations\nGuides\nAPI\nMore\nðŸ¦œï¸ðŸ”—\nChat\nSearch\nâŒ˜\nK\nGet started\nIntroduction\nInstallation\nQuickstart\nSecurity\nLangChain Expression Language\nGet started\nWhy use LCEL\nInterface\nHow to\nCookbook\nModules\nModel I/O\nRetrieval\nAgents\nMore\nLangServe\nLangSmith\nGet startedQuickstart\nQuickstart\n\nIn this quickstart we'll show you how to:\n\nGet setup with LangChain, LangSmith and LangServe\nUse the most basic and common components of LangChain: prompt templates, models, and output parsers\nUse LangChain Expression Language, the protocol that LangChain is built on and which facilitates component chaining\nBuild a simple application with LangChain\nTrace your application with LangSmith\nServe your application with LangServe\n\nThat's a fair amount to cover! Let's dive in.\n\nSetupâ€‹\nInstallationâ€‹\n\nTo install LangChain run:\n\nPip\nConda\npip install langchain\n\n\nFor more details, see our Installation guide.\n\nEnvironmentâ€‹\n\nUsing LangChain will usually require integrations with one or more model providers, data stores, APIs, etc. For this example, we'll use OpenAI's model APIs.\n\nFirst we'll need to install their Python package:\n\npip install openai\n\n\nAccessing the API requires an API key, which you can get by creating an account and heading here. Once we have a key we'll want to set it as an environment variable by running:\n\nexport OPENAI_API_KEY=\"...\"\n\n\nIf you'd prefer not to set an environment variable you can pass the key in directly via the openai_api_key named parameter when initiating the OpenAI LLM class:\n\nfrom langchain.chat_models import ChatOpenAI\n\nllm = ChatOpenAI(openai_api_key=\"...\")\n\nLangSmithâ€‹\n\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith.\n\nNote that LangSmith is not needed, but it is helpful. If you do want to use LangSmith, after you sign up at the link above, make sure to set your environment variables to start logging traces:\n\nexport LANGCHAIN_TRACING_V2=\"true\"\nexport LANGCHAIN_API_KEY=...\n\nLangServeâ€‹\n\nLangServe helps developers deploy LangChain chains as a REST API. You do not need to use LangServe to use LangChain, but in this guide we'll show how you can deploy your app with LangServe.\n\nInstall with:\n\npip install \"langserve[all]\"\n\nBuilding with LangChainâ€‹\n\nLangChain provides many modules that can be used to build language model applications. Modules can be used as standalones in simple applications and they can be composed for more complex use cases. Composition is powered by LangChain Expression Language (LCEL), which defines a unified Runnable interface that many modules implement, making it possible to seamlessly chain components.\n\nThe simplest and most common chain contains three things:\n\nLLM/Chat Model: The language model is the core reasoning engine here. In order to work with LangChain, you need to understand the different types of language models and how to work with them.\nPrompt Template: This provides instructions to the language model. This controls what the language model outputs, so understanding how to construct prompts and different prompting strategies is crucial.\nOutput Parser: These translate the raw response from the language model to a more workable format, making it easy to use the output downstream.\n\nIn this guide we'll cover those three components individually, and then go over how to combine them. Understanding these concepts will set you up well for being able to use and customize LangChain applications. Most LangChain applications allow you to configure the model and/or the prompt, so knowing how to take advantage of this will be a big enabler.\n\nLLM / Chat Modelâ€‹\n\nThere are two types of language models:\n\nLLM: underlying model takes a string as input and returns a string\nChatModel: underlying model takes a list of messages as input and returns a message\n\nStrings are simple, but what exactly are messages? The base message interface is defined by BaseMessage, which has two required attributes:\n\ncontent: The content of the message. Usually a string.\nrole: The entity from which the BaseMessage is coming.\n\nLangChain provides several objects to easily distinguish between different roles:\n\nHumanMessage: A BaseMessage coming from a human/user.\nAIMessage: A BaseMessage coming from an AI/assistant.\nSystemMessage: A BaseMessage coming from the system.\nFunctionMessage / ToolMessage: A BaseMessage containing the output of a function or tool call.\n\nIf none of those roles sound right, there is also a ChatMessage class where you can specify the role manually.\n\nLangChain provides a common interface that's shared by both LLMs and ChatModels. However it's useful to understand the difference in order to most effectively construct prompts for a given language model.\n\nThe simplest way to call an LLM or ChatModel is using .invoke(), the universal synchronous call method for all LangChain Expression Language (LCEL) objects:\n\nLLM.invoke: Takes in a string, returns a string.\nChatModel.invoke: Takes in a list of BaseMessage, returns a BaseMessage.\n\nThe input types for these methods are actually more general than this, but for simplicity here we can assume LLMs only take strings and Chat models only takes lists of messages. Check out the \"Go deeper\" section below to learn more about model invocation.\n\nLet's see how to work with these different types of models and these different types of inputs. First, let's import an LLM and a ChatModel.\n\nfrom langchain.llms import OpenAI\nfrom langchain.chat_models import ChatOpenAI\n\nllm = OpenAI()\nchat_model = ChatOpenAI()\n\n\nLLM and ChatModel objects are effectively configuration objects. You can initialize them with parameters like temperature and others, and pass them around.\n\nfrom langchain.schema import HumanMessage\n\ntext = \"What would be a good company name for a company that makes colorful socks?\"\nmessages = [HumanMessage(content=text)]\n\nllm.invoke(text)\n# >> Feetful of Fun\n\nchat_model.invoke(messages)\n# >> AIMessage(content=\"Socks O'Color\")\n\nGo deeper\nPrompt templatesâ€‹\n\nMost LLM applications do not pass user input directly into an LLM. Usually they will add the user input to a larger piece of text, called a prompt template, that provides additional context on the specific task at hand.\n\nIn the previous example, the text we passed to the model contained instructions to generate a company name. For our application, it would be great if the user only had to provide the description of a company/product, without having to worry about giving the model instructions.\n\nPromptTemplates help with exactly this! They bundle up all the logic for going from user input into a fully formatted prompt. This can start off very simple - for example, a prompt to produce the above string would just be:\n\nfrom langchain.prompts import PromptTemplate\n\nprompt = PromptTemplate.from_template(\"What is a good name for a company that makes {product}?\")\nprompt.format(product=\"colorful socks\")\n\nWhat is a good name for a company that makes colorful socks?\n\n\nHowever, the advantages of using these over raw string formatting are several. You can \"partial\" out variables - e.g. you can format only some of the variables at a time. You can compose them together, easily combining different templates into a single prompt. For explanations of these functionalities, see the section on prompts for more detail.\n\nPromptTemplates can also be used to produce a list of messages. In this case, the prompt not only contains information about the content, but also each message (its role, its position in the list, etc.). Here, what happens most often is a ChatPromptTemplate is a list of ChatMessageTemplates. Each ChatMessageTemplate contains instructions for how to format that ChatMessage - its role, and then also its content. Let's take a look at this below:\n\nfrom langchain.prompts.chat import ChatPromptTemplate\n\ntemplate = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\nhuman_template = \"{text}\"\n\nchat_prompt = ChatPromptTemplate.from_messages([\n    (\"system\", template),\n    (\"human\", human_template),\n])\n\nchat_prompt.format_messages(input_language=\"English\", output_language=\"French\", text=\"I love programming.\")\n\n[\n    SystemMessage(content=\"You are a helpful assistant that translates English to French.\", additional_kwargs={}),\n    HumanMessage(content=\"I love programming.\")\n]\n\n\nChatPromptTemplates can also be constructed in other ways - see the section on prompts for more detail.\n\nOutput parsersâ€‹\n\nOutputParsers convert the raw output of a language model into a format that can be used downstream. There are few main types of OutputParsers, including:\n\nConvert text from LLM into structured information (e.g. JSON)\nConvert a ChatMessage into just a string\nConvert the extra information returned from a call besides the message (like OpenAI function invocation) into a string.\n\nFor full information on this, see the section on output parsers.\n\nIn this getting started guide, we will write our own output parser - one that converts a comma separated list into a list.\n\nfrom langchain.schema import BaseOutputParser\n\nclass CommaSeparatedListOutputParser(BaseOutputParser):\n    \"\"\"Parse the output of an LLM call to a comma-separated list.\"\"\"\n\n\n    def parse(self, text: str):\n        \"\"\"Parse the output of an LLM call.\"\"\"\n        return text.strip().split(\", \")\n\nCommaSeparatedListOutputParser().parse(\"hi, bye\")\n# >> ['hi', 'bye']\n\nComposing with LCELâ€‹\n\nWe can now combine all these into one chain. This chain will take input variables, pass those to a prompt template to create a prompt, pass the prompt to a language model, and then pass the output through an (optional) output parser. This is a convenient way to bundle up a modular piece of logic. Let's see it in action!\n\nfrom typing import List\n\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain.schema import BaseOutputParser\n\nclass CommaSeparatedListOutputParser(BaseOutputParser[List[str]]):\n    \"\"\"Parse the output of an LLM call to a comma-separated list.\"\"\"\n\n\n    def parse(self, text: str) -> List[str]:\n        \"\"\"Parse the output of an LLM call.\"\"\"\n        return text.strip().split(\", \")\n\ntemplate = \"\"\"You are a helpful assistant who generates comma separated lists.\nA user will pass in a category, and you should generate 5 objects in that category in a comma separated list.\nONLY return a comma separated list, and nothing more.\"\"\"\nhuman_template = \"{text}\"\n\nchat_prompt = ChatPromptTemplate.from_messages([\n    (\"system\", template),\n    (\"human\", human_template),\n])\nchain = chat_prompt | ChatOpenAI() | CommaSeparatedListOutputParser()\nchain.invoke({\"text\": \"colors\"})\n# >> ['red', 'blue', 'green', 'yellow', 'orange']\n\n\nNote that we are using the | syntax to join these components together. This | syntax is powered by the LangChain Expression Language (LCEL) and relies on the universal Runnable interface that all of these objects implement. To learn more about LCEL, read the documentation here.\n\nTracing with LangSmithâ€‹\n\nAssuming we've set our environment variables as shown in the beginning, all of the model and chain calls we've been making will have been automatically logged to LangSmith. Once there, we can use LangSmith to debug and annotate our application traces, then turn them into datasets for evaluating future iterations of the application.\n\nCheck out what the trace for the above chain would look like: https://smith.langchain.com/public/09370280-4330-4eb4-a7e8-c91817f6aa13/r\n\nFor more on LangSmith head here.\n\nServing with LangServeâ€‹\n\nNow that we've built an application, we need to serve it. That's where LangServe comes in. LangServe helps developers deploy LCEL chains as a REST API. The library is integrated with FastAPI and uses pydantic for data validation.\n\nServerâ€‹\n\nTo create a server for our application we'll make a serve.py file with three things:\n\nThe definition of our chain (same as above)\nOur FastAPI app\nA definition of a route from which to serve the chain, which is done with langserve.add_routes\n#!/usr/bin/env python\nfrom typing import List\n\nfrom fastapi import FastAPI\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.schema import BaseOutputParser\nfrom langserve import add_routes\n\n# 1. Chain definition\n\nclass CommaSeparatedListOutputParser(BaseOutputParser[List[str]]):\n    \"\"\"Parse the output of an LLM call to a comma-separated list.\"\"\"\n\n\n    def parse(self, text: str) -> List[str]:\n        \"\"\"Parse the output of an LLM call.\"\"\"\n        return text.strip().split(\", \")\n\ntemplate = \"\"\"You are a helpful assistant who generates comma separated lists.\nA user will pass in a category, and you should generate 5 objects in that category in a comma separated list.\nONLY return a comma separated list, and nothing more.\"\"\"\nhuman_template = \"{text}\"\n\nchat_prompt = ChatPromptTemplate.from_messages([\n    (\"system\", template),\n    (\"human\", human_template),\n])\ncategory_chain = chat_prompt | ChatOpenAI() | CommaSeparatedListOutputParser()\n\n# 2. App definition\napp = FastAPI(\n  title=\"LangChain Server\",\n  version=\"1.0\",\n  description=\"A simple API server using LangChain's Runnable interfaces\",\n)\n\n# 3. Adding chain route\nadd_routes(\n    app,\n    category_chain,\n    path=\"/category_chain\",\n)\n\nif __name__ == \"__main__\":\n    import uvicorn\n\n    uvicorn.run(app, host=\"localhost\", port=8000)\n\n\nAnd that's it! If we execute this file:\n\npython serve.py\n\n\nwe should see our chain being served at localhost:8000.\n\nPlaygroundâ€‹\n\nEvery LangServe service comes with a simple built-in UI for configuring and invoking the application with streaming output and visibility into intermediate steps. Head to http://localhost:8000/category_chain/playground/ to try it out!\n\nClientâ€‹\n\nNow let's set up a client for programmatically interacting with our service. We can easily do this with the langserve.RemoteRunnable. Using this, we can interact with the served chain as if it were running client-side.\n\nfrom langserve import RemoteRunnable\n\nremote_chain = RemoteRunnable(\"http://localhost:8000/category_chain/\")\nremote_chain.invoke({\"text\": \"colors\"})\n# >> ['red', 'blue', 'green', 'yellow', 'orange']\n\n\nTo learn more about the many other features of LangServe head here.\n\nNext stepsâ€‹\n\nWe've touched on how to build an application with LangChain, how to trace it with LangSmith, and how to serve it with LangServe. There are a lot more features in all three of these than we can cover here. To continue on your journey:\n\nRead up on LangChain Expression Language (LCEL) to learn how to chain these components together\nDive deeper into LLMs, prompts, and output parsers and learn the other key components\nExplore common end-to-end use cases and template applications\nRead up on LangSmith, the platform for debugging, testing, monitoring and more\nLearn more about serving your applications with LangServe\nPrevious\nInstallation\nNext\nSecurity\nSetup\nInstallation\nEnvironment\nLangSmith\nLangServe\nBuilding with LangChain\nLLM / Chat Model\nPrompt templates\nOutput parsers\nComposing with LCEL\nTracing with LangSmith\nServing with LangServe\nServer\nPlayground\nClient\nNext steps\nCommunity\nDiscord\nTwitter\nGitHub\nPython\nJS/TS\nMore\nHomepage\nBlog\nCopyright Â© 2023 LangChain, Inc."
}