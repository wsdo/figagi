{
	"title": "Web scraping | ü¶úÔ∏èüîó Langchain",
	"url": "https://python.langchain.com/docs/use_cases/web_scraping",
	"html": "Skip to main content\nü¶úÔ∏èüîó LangChain\nDocs\nUse cases\nIntegrations\nGuides\nAPI\nMore\nü¶úÔ∏èüîó\nChat\nSearch\n‚åò\nK\nQA over structured data\nSQL\nRetrieval-augmented generation (RAG)\nInteracting with APIs\nChatbots\nExtraction\nSummarization\nTagging\nWeb scraping\nSynthetic data generation\nGraph querying\nWeb scraping\nWeb scraping\n\nOpen In Collab\n\nUse case‚Äã\n\nWeb research is one of the killer LLM applications:\n\nUsers have highlighted it as one of his top desired AI tools.\nOSS repos like gpt-researcher are growing in popularity.\n\nOverview‚Äã\n\nGathering content from the web has a few components:\n\nSearch: Query to url (e.g., using GoogleSearchAPIWrapper).\nLoading: Url to HTML (e.g., using AsyncHtmlLoader, AsyncChromiumLoader, etc).\nTransforming: HTML to formatted text (e.g., using HTML2Text or Beautiful Soup).\nQuickstart‚Äã\npip install -q openai langchain playwright beautifulsoup4\nplaywright install\n\n# Set env var OPENAI_API_KEY or load from a .env file:\n# import dotenv\n# dotenv.load_dotenv()\n\n\nScraping HTML content using a headless instance of Chromium.\n\nThe async nature of the scraping process is handled using Python‚Äôs asyncio library.\nThe actual interaction with the web pages is handled by Playwright.\nfrom langchain.document_loaders import AsyncChromiumLoader\nfrom langchain.document_transformers import BeautifulSoupTransformer\n\n# Load HTML\nloader = AsyncChromiumLoader([\"https://www.wsj.com\"])\nhtml = loader.load()\n\n\nScrape text content tags such as <p>, <li>, <div>, and <a> tags from the HTML content:\n\n<p>: The paragraph tag. It defines a paragraph in HTML and is used to group together related sentences and/or phrases.\n\n<li>: The list item tag. It is used within ordered (<ol>) and unordered (<ul>) lists to define individual items within the list.\n\n<div>: The division tag. It is a block-level element used to group other inline or block-level elements.\n\n<a>: The anchor tag. It is used to define hyperlinks.\n\n<span>: an inline container used to mark up a part of a text, or a part of a document.\n\nFor many news websites (e.g., WSJ, CNN), headlines and summaries are all in <span> tags.\n\n# Transform\nbs_transformer = BeautifulSoupTransformer()\ndocs_transformed = bs_transformer.transform_documents(html, tags_to_extract=[\"span\"])\n\n# Result\ndocs_transformed[0].page_content[0:500]\n\n'English EditionEnglish‰∏≠Êñá (Chinese)Êó•Êú¨Ë™û (Japanese) More Other Products from WSJBuy Side from WSJWSJ ShopWSJ Wine Other Products from WSJ Search Quotes and Companies Search Quotes and Companies 0.15% 0.03% 0.12% -0.42% 4.102% -0.69% -0.25% -0.15% -1.82% 0.24% 0.19% -1.10% About Evan His Family Reflects His Reporting How You Can Help Write a Message Life in Detention Latest News Get Email Updates Four Americans Released From Iranian Prison The Americans will remain under house arrest until they are '\n\n\nThese Documents now are staged for downstream usage in various LLM apps, as discussed below.\n\nLoader‚Äã\nAsyncHtmlLoader‚Äã\n\nThe AsyncHtmlLoader uses the aiohttp library to make asynchronous HTTP requests, suitable for simpler and lightweight scraping.\n\nAsyncChromiumLoader‚Äã\n\nThe AsyncChromiumLoader uses Playwright to launch a Chromium instance, which can handle JavaScript rendering and more complex web interactions.\n\nChromium is one of the browsers supported by Playwright, a library used to control browser automation.\n\nHeadless mode means that the browser is running without a graphical user interface, which is commonly used for web scraping.\n\nfrom langchain.document_loaders import AsyncHtmlLoader\n\nurls = [\"https://www.espn.com\", \"https://lilianweng.github.io/posts/2023-06-23-agent/\"]\nloader = AsyncHtmlLoader(urls)\ndocs = loader.load()\n\nTransformer‚Äã\nHTML2Text‚Äã\n\nHTML2Text provides a straightforward conversion of HTML content into plain text (with markdown-like formatting) without any specific tag manipulation.\n\nIt‚Äôs best suited for scenarios where the goal is to extract human-readable text without needing to manipulate specific HTML elements.\n\nBeautiful Soup‚Äã\n\nBeautiful Soup offers more fine-grained control over HTML content, enabling specific tag extraction, removal, and content cleaning.\n\nIt‚Äôs suited for cases where you want to extract specific information and clean up the HTML content according to your needs.\n\nfrom langchain.document_loaders import AsyncHtmlLoader\n\nurls = [\"https://www.espn.com\", \"https://lilianweng.github.io/posts/2023-06-23-agent/\"]\nloader = AsyncHtmlLoader(urls)\ndocs = loader.load()\n\nFetching pages: 100%|#############################################################################################################| 2/2 [00:00<00:00,  7.01it/s]\n\nfrom langchain.document_transformers import Html2TextTransformer\n\nhtml2text = Html2TextTransformer()\ndocs_transformed = html2text.transform_documents(docs)\ndocs_transformed[0].page_content[0:500]\n\n\"Skip to main content  Skip to navigation\\n\\n<\\n\\n>\\n\\nMenu\\n\\n## ESPN\\n\\n  * Search\\n\\n  *   * scores\\n\\n  * NFL\\n  * MLB\\n  * NBA\\n  * NHL\\n  * Soccer\\n  * NCAAF\\n  * ‚Ä¶\\n\\n    * Women's World Cup\\n    * LLWS\\n    * NCAAM\\n    * NCAAW\\n    * Sports Betting\\n    * Boxing\\n    * CFL\\n    * NCAA\\n    * Cricket\\n    * F1\\n    * Golf\\n    * Horse\\n    * MMA\\n    * NASCAR\\n    * NBA G League\\n    * Olympic Sports\\n    * PLL\\n    * Racing\\n    * RN BB\\n    * RN FB\\n    * Rugby\\n    * Tennis\\n    * WNBA\\n    * WWE\\n    * X Games\\n    * XFL\\n\\n  * More\"\n\nScraping with extraction‚Äã\nLLM with function calling‚Äã\n\nWeb scraping is challenging for many reasons.\n\nOne of them is the changing nature of modern websites‚Äô layouts and content, which requires modifying scraping scripts to accommodate the changes.\n\nUsing Function (e.g., OpenAI) with an extraction chain, we avoid having to change your code constantly when websites change.\n\nWe‚Äôre using gpt-3.5-turbo-0613 to guarantee access to OpenAI Functions feature (although this might be available to everyone by time of writing).\n\nWe‚Äôre also keeping temperature at 0 to keep randomness of the LLM down.\n\nfrom langchain.chat_models import ChatOpenAI\n\nllm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-0613\")\n\nDefine a schema‚Äã\n\nNext, you define a schema to specify what kind of data you want to extract.\n\nHere, the key names matter as they tell the LLM what kind of information they want.\n\nSo, be as detailed as possible.\n\nIn this example, we want to scrape only news article‚Äôs name and summary from The Wall Street Journal website.\n\nfrom langchain.chains import create_extraction_chain\n\nschema = {\n    \"properties\": {\n        \"news_article_title\": {\"type\": \"string\"},\n        \"news_article_summary\": {\"type\": \"string\"},\n    },\n    \"required\": [\"news_article_title\", \"news_article_summary\"],\n}\n\n\ndef extract(content: str, schema: dict):\n    return create_extraction_chain(schema=schema, llm=llm).run(content)\n\nRun the web scraper w/ BeautifulSoup‚Äã\n\nAs shown above, we‚Äôll be using BeautifulSoupTransformer.\n\nimport pprint\n\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\n\ndef scrape_with_playwright(urls, schema):\n    loader = AsyncChromiumLoader(urls)\n    docs = loader.load()\n    bs_transformer = BeautifulSoupTransformer()\n    docs_transformed = bs_transformer.transform_documents(\n        docs, tags_to_extract=[\"span\"]\n    )\n    print(\"Extracting content with LLM\")\n\n    # Grab the first 1000 tokens of the site\n    splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n        chunk_size=1000, chunk_overlap=0\n    )\n    splits = splitter.split_documents(docs_transformed)\n\n    # Process the first split\n    extracted_content = extract(schema=schema, content=splits[0].page_content)\n    pprint.pprint(extracted_content)\n    return extracted_content\n\n\nurls = [\"https://www.wsj.com\"]\nextracted_content = scrape_with_playwright(urls, schema=schema)\n\nExtracting content with LLM\n[{'news_article_summary': 'The Americans will remain under house arrest until '\n                          'they are allowed to return to the U.S. in coming '\n                          'weeks, following a monthslong diplomatic push by '\n                          'the Biden administration.',\n  'news_article_title': 'Four Americans Released From Iranian Prison'},\n {'news_article_summary': 'Price pressures continued cooling last month, with '\n                          'the CPI rising a mild 0.2% from June, likely '\n                          'deterring the Federal Reserve from raising interest '\n                          'rates at its September meeting.',\n  'news_article_title': 'Cooler July Inflation Opens Door to Fed Pause on '\n                        'Rates'},\n {'news_article_summary': 'The company has decided to eliminate 27 of its 30 '\n                          'clothing labels, such as Lark & Ro and Goodthreads, '\n                          'as it works to fend off antitrust scrutiny and cut '\n                          'costs.',\n  'news_article_title': 'Amazon Cuts Dozens of House Brands'},\n {'news_article_summary': 'President Biden‚Äôs order comes on top of a slowing '\n                          'Chinese economy, Covid lockdowns and rising '\n                          'tensions between the two powers.',\n  'news_article_title': 'U.S. Investment Ban on China Poised to Deepen Divide'},\n {'news_article_summary': 'The proposed trial date in the '\n                          'election-interference case comes on the same day as '\n                          'the former president‚Äôs not guilty plea on '\n                          'additional Mar-a-Lago charges.',\n  'news_article_title': 'Trump Should Be Tried in January, Prosecutors Tell '\n                        'Judge'},\n {'news_article_summary': 'The CEO who started in June says the platform has '\n                          '‚Äúan entirely different road map‚Äù for the future.',\n  'news_article_title': 'Yaccarino Says X Is Watching Threads but Has Its Own '\n                        'Vision'},\n {'news_article_summary': 'Students foot the bill for flagship state '\n                          'universities that pour money into new buildings and '\n                          'programs with little pushback.',\n  'news_article_title': 'Colleges Spend Like There‚Äôs No Tomorrow. ‚ÄòThese '\n                        'Places Are Just Devouring Money.‚Äô'},\n {'news_article_summary': 'Wildfires fanned by hurricane winds have torn '\n                          'through parts of the Hawaiian island, devastating '\n                          'the popular tourist town of Lahaina.',\n  'news_article_title': 'Maui Wildfires Leave at Least 36 Dead'},\n {'news_article_summary': 'After its large armored push stalled, Kyiv has '\n                          'fallen back on the kind of tactics that brought it '\n                          'success earlier in the war.',\n  'news_article_title': 'Ukraine Uses Small-Unit Tactics to Retake Captured '\n                        'Territory'},\n {'news_article_summary': 'President Guillermo Lasso says the Aug. 20 election '\n                          'will proceed, as the Andean country grapples with '\n                          'rising drug gang violence.',\n  'news_article_title': 'Ecuador Declares State of Emergency After '\n                        'Presidential Hopeful Killed'},\n {'news_article_summary': 'This year‚Äôs hurricane season, which typically runs '\n                          'from June to the end of November, has been '\n                          'difficult to predict, climate scientists said.',\n  'news_article_title': 'Atlantic Hurricane Season Prediction Increased to '\n                        '‚ÄòAbove Normal,‚Äô NOAA Says'},\n {'news_article_summary': 'The NFL is raising the price of its NFL+ streaming '\n                          'packages as it adds the NFL Network and RedZone.',\n  'news_article_title': 'NFL to Raise Price of NFL+ Streaming Packages as It '\n                        'Adds NFL Network, RedZone'},\n {'news_article_summary': 'Russia is planning a moon mission as part of the '\n                          'new space race.',\n  'news_article_title': 'Russia‚Äôs Moon Mission and the New Space Race'},\n {'news_article_summary': 'Tapestry‚Äôs $8.5 billion acquisition of Capri would '\n                          'create a conglomerate with more than $12 billion in '\n                          'annual sales, but it would still lack the '\n                          'high-wattage labels and diversity that have fueled '\n                          'LVMH‚Äôs success.',\n  'news_article_title': \"Why the Coach and Kors Marriage Doesn't Scare LVMH\"},\n {'news_article_summary': 'The Supreme Court has blocked Purdue Pharma‚Äôs $6 '\n                          'billion Sackler opioid settlement.',\n  'news_article_title': 'Supreme Court Blocks Purdue Pharma‚Äôs $6 Billion '\n                        'Sackler Opioid Settlement'},\n {'news_article_summary': 'The Social Security COLA is expected to rise in '\n                          '2024, but not by a lot.',\n  'news_article_title': 'Social Security COLA Expected to Rise in 2024, but '\n                        'Not by a Lot'}]\n\n\nWe can compare the headlines scraped to the page:\n\nLooking at the LangSmith trace, we can see what is going on under the hood:\n\nIt‚Äôs following what is explained in the extraction.\nWe call the information_extraction function on the input text.\nIt will attempt to populate the provided schema from the url content.\nResearch automation‚Äã\n\nRelated to scraping, we may want to answer specific questions using searched content.\n\nWe can automate the process of web research using a retriever, such as the WebResearchRetriever (docs).\n\nCopy requirements from here:\n\npip install -r requirements.txt\n\nSet GOOGLE_CSE_ID and GOOGLE_API_KEY.\n\nfrom langchain.chat_models.openai import ChatOpenAI\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.retrievers.web_research import WebResearchRetriever\nfrom langchain.utilities import GoogleSearchAPIWrapper\nfrom langchain.vectorstores import Chroma\n\n# Vectorstore\nvectorstore = Chroma(\n    embedding_function=OpenAIEmbeddings(), persist_directory=\"./chroma_db_oai\"\n)\n\n# LLM\nllm = ChatOpenAI(temperature=0)\n\n# Search\nsearch = GoogleSearchAPIWrapper()\n\n\nInitialize retriever with the above tools to:\n\nUse an LLM to generate multiple relevant search queries (one LLM call)\nExecute a search for each query\nChoose the top K links per query (multiple search calls in parallel)\nLoad the information from all chosen links (scrape pages in parallel)\nIndex those documents into a vectorstore\nFind the most relevant documents for each original generated search query\n# Initialize\nweb_research_retriever = WebResearchRetriever.from_llm(\n    vectorstore=vectorstore, llm=llm, search=search\n)\n\n# Run\nimport logging\n\nlogging.basicConfig()\nlogging.getLogger(\"langchain.retrievers.web_research\").setLevel(logging.INFO)\nfrom langchain.chains import RetrievalQAWithSourcesChain\n\nuser_input = \"How do LLM Powered Autonomous Agents work?\"\nqa_chain = RetrievalQAWithSourcesChain.from_chain_type(\n    llm, retriever=web_research_retriever\n)\nresult = qa_chain({\"question\": user_input})\nresult\n\nINFO:langchain.retrievers.web_research:Generating questions for Google Search ...\nINFO:langchain.retrievers.web_research:Questions for Google Search (raw): {'question': 'How do LLM Powered Autonomous Agents work?', 'text': LineList(lines=['1. What is the functioning principle of LLM Powered Autonomous Agents?\\n', '2. How do LLM Powered Autonomous Agents operate?\\n'])}\nINFO:langchain.retrievers.web_research:Questions for Google Search: ['1. What is the functioning principle of LLM Powered Autonomous Agents?\\n', '2. How do LLM Powered Autonomous Agents operate?\\n']\nINFO:langchain.retrievers.web_research:Searching for relevant urls ...\nINFO:langchain.retrievers.web_research:Searching for relevant urls ...\nINFO:langchain.retrievers.web_research:Search results: [{'title': 'LLM Powered Autonomous Agents | Hacker News', 'link': 'https://news.ycombinator.com/item?id=36488871', 'snippet': 'Jun 26, 2023 ... Exactly. A temperature of 0 means you always pick the highest probability token (i.e. the \"max\" function), while a temperature of 1 means you\\xa0...'}]\nINFO:langchain.retrievers.web_research:Searching for relevant urls ...\nINFO:langchain.retrievers.web_research:Search results: [{'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'link': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'snippet': 'Jun 23, 2023 ... Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\" , \"What are the subgoals for achieving XYZ?\" , (2) by\\xa0...'}]\nINFO:langchain.retrievers.web_research:New URLs to load: []\nINFO:langchain.retrievers.web_research:Grabbing most relevant splits from urls...\n\n{'question': 'How do LLM Powered Autonomous Agents work?',\n 'answer': \"LLM-powered autonomous agents work by using LLM as the agent's brain, complemented by several key components such as planning, memory, and tool use. In terms of planning, the agent breaks down large tasks into smaller subgoals and can reflect and refine its actions based on past experiences. Memory is divided into short-term memory, which is used for in-context learning, and long-term memory, which allows the agent to retain and recall information over extended periods. Tool use involves the agent calling external APIs for additional information. These agents have been used in various applications, including scientific discovery and generative agents simulation.\",\n 'sources': ''}\n\nGoing deeper‚Äã\nHere‚Äôs a app that wraps this retriever with a lighweight UI.\nQuestion answering over a website‚Äã\n\nTo answer questions over a specific website, you can use Apify‚Äôs Website Content Crawler Actor, which can deeply crawl websites such as documentation, knowledge bases, help centers, or blogs, and extract text content from the web pages.\n\nIn the example below, we will deeply crawl the Python documentation of LangChain‚Äôs Chat LLM models and answer a question over it.\n\nFirst, install the requirements pip install apify-client openai langchain chromadb tiktoken\n\nNext, set OPENAI_API_KEY and APIFY_API_TOKEN in your environment variables.\n\nThe full code follows:\n\nfrom langchain.docstore.document import Document\nfrom langchain.indexes import VectorstoreIndexCreator\nfrom langchain.utilities import ApifyWrapper\n\napify = ApifyWrapper()\n# Call the Actor to obtain text from the crawled webpages\nloader = apify.call_actor(\n    actor_id=\"apify/website-content-crawler\",\n    run_input={\n        \"startUrls\": [{\"url\": \"https://python.langchain.com/docs/integrations/chat/\"}]\n    },\n    dataset_mapping_function=lambda item: Document(\n        page_content=item[\"text\"] or \"\", metadata={\"source\": item[\"url\"]}\n    ),\n)\n\n# Create a vector store based on the crawled data\nindex = VectorstoreIndexCreator().from_loaders([loader])\n\n# Query the vector store\nquery = \"Are any OpenAI chat models integrated in LangChain?\"\nresult = index.query(query)\nprint(result)\n\n Yes, LangChain offers integration with OpenAI chat models. You can use the ChatOpenAI class to interact with OpenAI models.\n\nPrevious\nTagging\nNext\nSynthetic data generation\nUse case\nOverview\nQuickstart\nLoader\nAsyncHtmlLoader\nAsyncChromiumLoader\nTransformer\nHTML2Text\nBeautiful Soup\nScraping with extraction\nLLM with function calling\nDefine a schema\nRun the web scraper w/ BeautifulSoup\nResearch automation\nGoing deeper\nQuestion answering over a website\nCommunity\nDiscord\nTwitter\nGitHub\nPython\nJS/TS\nMore\nHomepage\nBlog\nCopyright ¬© 2023 LangChain, Inc."
}