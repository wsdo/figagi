{
	"title": "LLMs | 🦜️🔗 Langchain",
	"url": "https://python.langchain.com/docs/integrations/llms/",
	"html": "Skip to main content\n🦜️🔗 LangChain\nDocs\nUse cases\nIntegrations\nGuides\nAPI\nMore\n🦜️🔗\nChat\nSearch\n⌘\nK\nProviders\nAnthropic\nAWS\nGoogle\nHugging Face\nMicrosoft\nOpenAI\nMore\nComponents\nLLMs\nAI21\nAleph Alpha\nAmazon API Gateway\nAnyscale\nArcee\nAzure ML\nAzure OpenAI\nBaidu Qianfan\nBanana\nBaseten\nBeam\nBedrock\nBittensor\nCerebriumAI\nChatGLM\nClarifai\nCloudflare Workers AI\nCohere\nC Transformers\nCTranslate2\nDatabricks\nDeepInfra\nDeepSparse\nEden AI\nFireworks\nForefrontAI\nGigaChat\nGoogle Cloud Vertex AI\nGooseAI\nGPT4All\nGradient\nHugging Face Hub\nHugging Face Local Pipelines\nHuggingface TextGen Inference\nJavelin AI Gateway Tutorial\nJSONFormer\nKoboldAI API\nLlama.cpp\nLLM Caching integrations\nLM Format Enforcer\nManifest\nMinimax\nModal\nMosaicML\nNLP Cloud\nOctoAI\nOllama\nOpaquePrompts\nOpenAI\nOpenLLM\nOpenLM\nAliCloud PAI EAS\nPetals\nPipelineAI\nPredibase\nPrediction Guard\nPromptLayer OpenAI\nRELLM\nReplicate\nRunhouse\nSageMakerEndpoint\nStochasticAI\nNebula (Symbl.ai)\nTextGen\nTitan Takeoff\nTitan Takeoff Pro\nTogether AI\nTongyi Qwen\nvLLM\nVolc Engine Maas\nWatsonxLLM\nWriter\nXorbits Inference (Xinference)\nYandexGPT\nChat models\nDocument loaders\nDocument transformers\nText embedding models\nVector stores\nRetrievers\nTools\nAgents and toolkits\nMemory\nCallbacks\nChat loaders\nAdapters\nStores\nComponentsLLMs\nLLMs\nFeatures (natively supported)​\n\nAll LLMs implement the Runnable interface, which comes with default implementations of all methods, ie. ainvoke, batch, abatch, stream, astream. This gives all LLMs basic support for async, streaming and batch, which by default is implemented as below:\n\nAsync support defaults to calling the respective sync method in asyncio's default thread pool executor. This lets other async functions in your application make progress while the LLM is being executed, by moving this call to a background thread.\nStreaming support defaults to returning an Iterator (or AsyncIterator in the case of async streaming) of a single value, the final result returned by the underlying LLM provider. This obviously doesn't give you token-by-token streaming, which requires native support from the LLM provider, but ensures your code that expects an iterator of tokens can work for any of our LLM integrations.\nBatch support defaults to calling the underlying LLM in parallel for each input by making use of a thread pool executor (in the sync batch case) or asyncio.gather (in the async batch case). The concurrency can be controlled with the max_concurrency key in RunnableConfig.\n\nEach LLM integration can optionally provide native implementations for async, streaming or batch, which, for providers that support it, can be more efficient. The table shows, for each integration, which features have been implemented with native support.\n\nModel\tInvoke\tAsync invoke\tStream\tAsync stream\tBatch\tAsync batch\nAI21\t✅\t❌\t❌\t❌\t❌\t❌\nAlephAlpha\t✅\t❌\t❌\t❌\t❌\t❌\nAmazonAPIGateway\t✅\t❌\t❌\t❌\t❌\t❌\nAnthropic\t✅\t✅\t✅\t✅\t❌\t❌\nAnyscale\t✅\t✅\t✅\t✅\t✅\t✅\nArcee\t✅\t❌\t❌\t❌\t❌\t❌\nAviary\t✅\t❌\t❌\t❌\t❌\t❌\nAzureMLOnlineEndpoint\t✅\t❌\t❌\t❌\t❌\t❌\nAzureOpenAI\t✅\t✅\t✅\t✅\t✅\t✅\nBanana\t✅\t❌\t❌\t❌\t❌\t❌\nBaseten\t✅\t❌\t❌\t❌\t❌\t❌\nBeam\t✅\t❌\t❌\t❌\t❌\t❌\nBedrock\t✅\t❌\t✅\t❌\t❌\t❌\nCTransformers\t✅\t✅\t❌\t❌\t❌\t❌\nCTranslate2\t✅\t❌\t❌\t❌\t✅\t❌\nCerebriumAI\t✅\t❌\t❌\t❌\t❌\t❌\nChatGLM\t✅\t❌\t❌\t❌\t❌\t❌\nClarifai\t✅\t❌\t❌\t❌\t❌\t❌\nCohere\t✅\t✅\t❌\t❌\t❌\t❌\nDatabricks\t✅\t❌\t❌\t❌\t❌\t❌\nDeepInfra\t✅\t✅\t✅\t✅\t❌\t❌\nDeepSparse\t✅\t✅\t✅\t✅\t❌\t❌\nEdenAI\t✅\t✅\t❌\t❌\t❌\t❌\nFireworks\t✅\t✅\t✅\t✅\t✅\t✅\nForefrontAI\t✅\t❌\t❌\t❌\t❌\t❌\nGPT4All\t✅\t❌\t❌\t❌\t❌\t❌\nGigaChat\t✅\t✅\t✅\t✅\t✅\t✅\nGooglePalm\t✅\t❌\t❌\t❌\t✅\t❌\nGooseAI\t✅\t❌\t❌\t❌\t❌\t❌\nGradientLLM\t✅\t✅\t❌\t❌\t✅\t✅\nHuggingFaceEndpoint\t✅\t❌\t❌\t❌\t❌\t❌\nHuggingFaceHub\t✅\t❌\t❌\t❌\t❌\t❌\nHuggingFacePipeline\t✅\t❌\t❌\t❌\t✅\t❌\nHuggingFaceTextGenInference\t✅\t✅\t✅\t✅\t❌\t❌\nHumanInputLLM\t✅\t❌\t❌\t❌\t❌\t❌\nJavelinAIGateway\t✅\t✅\t❌\t❌\t❌\t❌\nKoboldApiLLM\t✅\t❌\t❌\t❌\t❌\t❌\nLlamaCpp\t✅\t❌\t✅\t❌\t❌\t❌\nManifestWrapper\t✅\t❌\t❌\t❌\t❌\t❌\nMinimax\t✅\t❌\t❌\t❌\t❌\t❌\nMlflowAIGateway\t✅\t❌\t❌\t❌\t❌\t❌\nModal\t✅\t❌\t❌\t❌\t❌\t❌\nMosaicML\t✅\t❌\t❌\t❌\t❌\t❌\nNIBittensorLLM\t✅\t❌\t❌\t❌\t❌\t❌\nNLPCloud\t✅\t❌\t❌\t❌\t❌\t❌\nNebula\t✅\t❌\t❌\t❌\t❌\t❌\nOctoAIEndpoint\t✅\t❌\t❌\t❌\t❌\t❌\nOllama\t✅\t❌\t❌\t❌\t❌\t❌\nOpaquePrompts\t✅\t❌\t❌\t❌\t❌\t❌\nOpenAI\t✅\t✅\t✅\t✅\t✅\t✅\nOpenLLM\t✅\t✅\t❌\t❌\t❌\t❌\nOpenLM\t✅\t✅\t✅\t✅\t✅\t✅\nPaiEasEndpoint\t✅\t❌\t✅\t❌\t❌\t❌\nPetals\t✅\t❌\t❌\t❌\t❌\t❌\nPipelineAI\t✅\t❌\t❌\t❌\t❌\t❌\nPredibase\t✅\t❌\t❌\t❌\t❌\t❌\nPredictionGuard\t✅\t❌\t❌\t❌\t❌\t❌\nPromptLayerOpenAI\t✅\t❌\t❌\t❌\t❌\t❌\nQianfanLLMEndpoint\t✅\t✅\t✅\t✅\t❌\t❌\nRWKV\t✅\t❌\t❌\t❌\t❌\t❌\nReplicate\t✅\t❌\t✅\t❌\t❌\t❌\nSagemakerEndpoint\t✅\t❌\t❌\t❌\t❌\t❌\nSelfHostedHuggingFaceLLM\t✅\t❌\t❌\t❌\t❌\t❌\nSelfHostedPipeline\t✅\t❌\t❌\t❌\t❌\t❌\nStochasticAI\t✅\t❌\t❌\t❌\t❌\t❌\nTextGen\t✅\t❌\t❌\t❌\t❌\t❌\nTitanTakeoff\t✅\t❌\t✅\t❌\t❌\t❌\nTitanTakeoffPro\t✅\t❌\t✅\t❌\t❌\t❌\nTongyi\t✅\t❌\t❌\t❌\t❌\t❌\nVLLM\t✅\t❌\t❌\t❌\t✅\t❌\nVLLMOpenAI\t✅\t✅\t✅\t✅\t✅\t✅\nVertexAI\t✅\t✅\t✅\t❌\t✅\t✅\nVertexAIModelGarden\t✅\t✅\t❌\t❌\t✅\t✅\nVolcEngineMaasLLM\t✅\t❌\t✅\t❌\t❌\t❌\nWatsonxLLM\t✅\t❌\t✅\t❌\t✅\t❌\nWriter\t✅\t❌\t❌\t❌\t❌\t❌\nXinference\t✅\t❌\t❌\t❌\t❌\t❌\nYandexGPT\t✅\t✅\t❌\t❌\t❌\t❌\nPrevious\nComponents\nNext\nLLMs\nFeatures (natively supported)\nCommunity\nDiscord\nTwitter\nGitHub\nPython\nJS/TS\nMore\nHomepage\nBlog\nCopyright © 2023 LangChain, Inc."
}