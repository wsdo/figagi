{
	"title": "Interface | ü¶úÔ∏èüîó Langchain",
	"url": "https://python.langchain.com/docs/expression_language/interface",
	"html": "Skip to main content\nü¶úÔ∏èüîó LangChain\nDocs\nUse cases\nIntegrations\nGuides\nAPI\nMore\nü¶úÔ∏èüîó\nChat\nSearch\n‚åò\nK\nGet started\nIntroduction\nInstallation\nQuickstart\nSecurity\nLangChain Expression Language\nGet started\nWhy use LCEL\nInterface\nHow to\nCookbook\nModules\nModel I/O\nRetrieval\nAgents\nMore\nLangServe\nLangSmith\nLangChain Expression LanguageInterface\nInterface\n\nTo make it as easy as possible to create custom chains, we‚Äôve implemented a ‚ÄúRunnable‚Äù protocol. The Runnable protocol is implemented for most components. This is a standard interface, which makes it easy to define custom chains as well as invoke them in a standard way. The standard interface includes:\n\nstream: stream back chunks of the response\ninvoke: call the chain on an input\nbatch: call the chain on a list of inputs\n\nThese also have corresponding async methods:\n\nastream: stream back chunks of the response async\nainvoke: call the chain on an input async\nabatch: call the chain on a list of inputs async\nastream_log: stream back intermediate steps as they happen, in addition to the final response\n\nThe input type and output type varies by component:\n\nComponent\tInput Type\tOutput Type\nPrompt\tDictionary\tPromptValue\nChatModel\tSingle string, list of chat messages or a PromptValue\tChatMessage\nLLM\tSingle string, list of chat messages or a PromptValue\tString\nOutputParser\tThe output of an LLM or ChatModel\tDepends on the parser\nRetriever\tSingle string\tList of Documents\nTool\tSingle string or dictionary, depending on the tool\tDepends on the tool\n\nAll runnables expose input and output schemas to inspect the inputs and outputs: - input_schema: an input Pydantic model auto-generated from the structure of the Runnable - output_schema: an output Pydantic model auto-generated from the structure of the Runnable\n\nLet‚Äôs take a look at these methods. To do so, we‚Äôll create a super simple PromptTemplate + ChatModel chain.\n\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.prompts import ChatPromptTemplate\n\nmodel = ChatOpenAI()\nprompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\nchain = prompt | model\n\nInput Schema‚Äã\n\nA description of the inputs accepted by a Runnable. This is a Pydantic model dynamically generated from the structure of any Runnable. You can call .schema() on it to obtain a JSONSchema representation.\n\n# The input schema of the chain is the input schema of its first part, the prompt.\nchain.input_schema.schema()\n\n{'title': 'PromptInput',\n 'type': 'object',\n 'properties': {'topic': {'title': 'Topic', 'type': 'string'}}}\n\nprompt.input_schema.schema()\n\n{'title': 'PromptInput',\n 'type': 'object',\n 'properties': {'topic': {'title': 'Topic', 'type': 'string'}}}\n\nmodel.input_schema.schema()\n\n{'title': 'ChatOpenAIInput',\n 'anyOf': [{'type': 'string'},\n  {'$ref': '#/definitions/StringPromptValue'},\n  {'$ref': '#/definitions/ChatPromptValueConcrete'},\n  {'type': 'array',\n   'items': {'anyOf': [{'$ref': '#/definitions/AIMessage'},\n     {'$ref': '#/definitions/HumanMessage'},\n     {'$ref': '#/definitions/ChatMessage'},\n     {'$ref': '#/definitions/SystemMessage'},\n     {'$ref': '#/definitions/FunctionMessage'}]}}],\n 'definitions': {'StringPromptValue': {'title': 'StringPromptValue',\n   'description': 'String prompt value.',\n   'type': 'object',\n   'properties': {'text': {'title': 'Text', 'type': 'string'},\n    'type': {'title': 'Type',\n     'default': 'StringPromptValue',\n     'enum': ['StringPromptValue'],\n     'type': 'string'}},\n   'required': ['text']},\n  'AIMessage': {'title': 'AIMessage',\n   'description': 'A Message from an AI.',\n   'type': 'object',\n   'properties': {'content': {'title': 'Content', 'type': 'string'},\n    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n    'type': {'title': 'Type',\n     'default': 'ai',\n     'enum': ['ai'],\n     'type': 'string'},\n    'example': {'title': 'Example', 'default': False, 'type': 'boolean'}},\n   'required': ['content']},\n  'HumanMessage': {'title': 'HumanMessage',\n   'description': 'A Message from a human.',\n   'type': 'object',\n   'properties': {'content': {'title': 'Content', 'type': 'string'},\n    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n    'type': {'title': 'Type',\n     'default': 'human',\n     'enum': ['human'],\n     'type': 'string'},\n    'example': {'title': 'Example', 'default': False, 'type': 'boolean'}},\n   'required': ['content']},\n  'ChatMessage': {'title': 'ChatMessage',\n   'description': 'A Message that can be assigned an arbitrary speaker (i.e. role).',\n   'type': 'object',\n   'properties': {'content': {'title': 'Content', 'type': 'string'},\n    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n    'type': {'title': 'Type',\n     'default': 'chat',\n     'enum': ['chat'],\n     'type': 'string'},\n    'role': {'title': 'Role', 'type': 'string'}},\n   'required': ['content', 'role']},\n  'SystemMessage': {'title': 'SystemMessage',\n   'description': 'A Message for priming AI behavior, usually passed in as the first of a sequence\\nof input messages.',\n   'type': 'object',\n   'properties': {'content': {'title': 'Content', 'type': 'string'},\n    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n    'type': {'title': 'Type',\n     'default': 'system',\n     'enum': ['system'],\n     'type': 'string'}},\n   'required': ['content']},\n  'FunctionMessage': {'title': 'FunctionMessage',\n   'description': 'A Message for passing the result of executing a function back to a model.',\n   'type': 'object',\n   'properties': {'content': {'title': 'Content', 'type': 'string'},\n    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n    'type': {'title': 'Type',\n     'default': 'function',\n     'enum': ['function'],\n     'type': 'string'},\n    'name': {'title': 'Name', 'type': 'string'}},\n   'required': ['content', 'name']},\n  'ChatPromptValueConcrete': {'title': 'ChatPromptValueConcrete',\n   'description': 'Chat prompt value which explicitly lists out the message types it accepts.\\nFor use in external schemas.',\n   'type': 'object',\n   'properties': {'messages': {'title': 'Messages',\n     'type': 'array',\n     'items': {'anyOf': [{'$ref': '#/definitions/AIMessage'},\n       {'$ref': '#/definitions/HumanMessage'},\n       {'$ref': '#/definitions/ChatMessage'},\n       {'$ref': '#/definitions/SystemMessage'},\n       {'$ref': '#/definitions/FunctionMessage'}]}},\n    'type': {'title': 'Type',\n     'default': 'ChatPromptValueConcrete',\n     'enum': ['ChatPromptValueConcrete'],\n     'type': 'string'}},\n   'required': ['messages']}}}\n\nOutput Schema‚Äã\n\nA description of the outputs produced by a Runnable. This is a Pydantic model dynamically generated from the structure of any Runnable. You can call .schema() on it to obtain a JSONSchema representation.\n\n# The output schema of the chain is the output schema of its last part, in this case a ChatModel, which outputs a ChatMessage\nchain.output_schema.schema()\n\n{'title': 'ChatOpenAIOutput',\n 'anyOf': [{'$ref': '#/definitions/HumanMessage'},\n  {'$ref': '#/definitions/AIMessage'},\n  {'$ref': '#/definitions/ChatMessage'},\n  {'$ref': '#/definitions/FunctionMessage'},\n  {'$ref': '#/definitions/SystemMessage'}],\n 'definitions': {'HumanMessage': {'title': 'HumanMessage',\n   'description': 'A Message from a human.',\n   'type': 'object',\n   'properties': {'content': {'title': 'Content', 'type': 'string'},\n    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n    'type': {'title': 'Type',\n     'default': 'human',\n     'enum': ['human'],\n     'type': 'string'},\n    'example': {'title': 'Example', 'default': False, 'type': 'boolean'}},\n   'required': ['content']},\n  'AIMessage': {'title': 'AIMessage',\n   'description': 'A Message from an AI.',\n   'type': 'object',\n   'properties': {'content': {'title': 'Content', 'type': 'string'},\n    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n    'type': {'title': 'Type',\n     'default': 'ai',\n     'enum': ['ai'],\n     'type': 'string'},\n    'example': {'title': 'Example', 'default': False, 'type': 'boolean'}},\n   'required': ['content']},\n  'ChatMessage': {'title': 'ChatMessage',\n   'description': 'A Message that can be assigned an arbitrary speaker (i.e. role).',\n   'type': 'object',\n   'properties': {'content': {'title': 'Content', 'type': 'string'},\n    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n    'type': {'title': 'Type',\n     'default': 'chat',\n     'enum': ['chat'],\n     'type': 'string'},\n    'role': {'title': 'Role', 'type': 'string'}},\n   'required': ['content', 'role']},\n  'FunctionMessage': {'title': 'FunctionMessage',\n   'description': 'A Message for passing the result of executing a function back to a model.',\n   'type': 'object',\n   'properties': {'content': {'title': 'Content', 'type': 'string'},\n    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n    'type': {'title': 'Type',\n     'default': 'function',\n     'enum': ['function'],\n     'type': 'string'},\n    'name': {'title': 'Name', 'type': 'string'}},\n   'required': ['content', 'name']},\n  'SystemMessage': {'title': 'SystemMessage',\n   'description': 'A Message for priming AI behavior, usually passed in as the first of a sequence\\nof input messages.',\n   'type': 'object',\n   'properties': {'content': {'title': 'Content', 'type': 'string'},\n    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n    'type': {'title': 'Type',\n     'default': 'system',\n     'enum': ['system'],\n     'type': 'string'}},\n   'required': ['content']}}}\n\nStream‚Äã\nfor s in chain.stream({\"topic\": \"bears\"}):\n    print(s.content, end=\"\", flush=True)\n\nWhy don't bears wear shoes?\n\nBecause they already have bear feet!\n\nInvoke‚Äã\nchain.invoke({\"topic\": \"bears\"})\n\nAIMessage(content=\"Why don't bears wear shoes?\\n\\nBecause they already have bear feet!\")\n\nBatch‚Äã\nchain.batch([{\"topic\": \"bears\"}, {\"topic\": \"cats\"}])\n\n[AIMessage(content=\"Why don't bears wear shoes?\\n\\nBecause they have bear feet!\"),\n AIMessage(content=\"Why don't cats play poker in the wild?\\n\\nToo many cheetahs!\")]\n\n\nYou can set the number of concurrent requests by using the max_concurrency parameter\n\nchain.batch([{\"topic\": \"bears\"}, {\"topic\": \"cats\"}], config={\"max_concurrency\": 5})\n\n[AIMessage(content=\"Why don't bears wear shoes? \\n\\nBecause they have bear feet!\"),\n AIMessage(content=\"Why don't cats play poker in the wild?\\n\\nToo many cheetahs!\")]\n\nAsync Stream‚Äã\nasync for s in chain.astream({\"topic\": \"bears\"}):\n    print(s.content, end=\"\", flush=True)\n\nSure, here's a bear-themed joke for you:\n\nWhy don't bears wear shoes?\n\nBecause they already have bear feet!\n\nAsync Invoke‚Äã\nawait chain.ainvoke({\"topic\": \"bears\"})\n\nAIMessage(content=\"Why don't bears wear shoes? \\n\\nBecause they have bear feet!\")\n\nAsync Batch‚Äã\nawait chain.abatch([{\"topic\": \"bears\"}])\n\n[AIMessage(content=\"Why don't bears wear shoes?\\n\\nBecause they have bear feet!\")]\n\nAsync Stream Intermediate Steps‚Äã\n\nAll runnables also have a method .astream_log() which is used to stream (as they happen) all or part of the intermediate steps of your chain/sequence.\n\nThis is useful to show progress to the user, to use intermediate results, or to debug your chain.\n\nYou can stream all steps (default) or include/exclude steps by name, tags or metadata.\n\nThis method yields JSONPatch ops that when applied in the same order as received build up the RunState.\n\nclass LogEntry(TypedDict):\n    id: str\n    \"\"\"ID of the sub-run.\"\"\"\n    name: str\n    \"\"\"Name of the object being run.\"\"\"\n    type: str\n    \"\"\"Type of the object being run, eg. prompt, chain, llm, etc.\"\"\"\n    tags: List[str]\n    \"\"\"List of tags for the run.\"\"\"\n    metadata: Dict[str, Any]\n    \"\"\"Key-value pairs of metadata for the run.\"\"\"\n    start_time: str\n    \"\"\"ISO-8601 timestamp of when the run started.\"\"\"\n\n    streamed_output_str: List[str]\n    \"\"\"List of LLM tokens streamed by this run, if applicable.\"\"\"\n    final_output: Optional[Any]\n    \"\"\"Final output of this run.\n    Only available after the run has finished successfully.\"\"\"\n    end_time: Optional[str]\n    \"\"\"ISO-8601 timestamp of when the run ended.\n    Only available after the run has finished.\"\"\"\n\n\nclass RunState(TypedDict):\n    id: str\n    \"\"\"ID of the run.\"\"\"\n    streamed_output: List[Any]\n    \"\"\"List of output chunks streamed by Runnable.stream()\"\"\"\n    final_output: Optional[Any]\n    \"\"\"Final output of the run, usually the result of aggregating (`+`) streamed_output.\n    Only available after the run has finished successfully.\"\"\"\n\n    logs: Dict[str, LogEntry]\n    \"\"\"Map of run names to sub-runs. If filters were supplied, this list will\n    contain only the runs that matched the filters.\"\"\"\n\nStreaming JSONPatch chunks‚Äã\n\nThis is useful eg. to stream the JSONPatch in an HTTP server, and then apply the ops on the client to rebuild the run state there. See LangServe for tooling to make it easier to build a webserver from any Runnable.\n\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.schema.output_parser import StrOutputParser\nfrom langchain.schema.runnable import RunnablePassthrough\nfrom langchain.vectorstores import FAISS\n\ntemplate = \"\"\"Answer the question based only on the following context:\n{context}\n\nQuestion: {question}\n\"\"\"\nprompt = ChatPromptTemplate.from_template(template)\n\nvectorstore = FAISS.from_texts(\n    [\"harrison worked at kensho\"], embedding=OpenAIEmbeddings()\n)\nretriever = vectorstore.as_retriever()\n\nretrieval_chain = (\n    {\n        \"context\": retriever.with_config(run_name=\"Docs\"),\n        \"question\": RunnablePassthrough(),\n    }\n    | prompt\n    | model\n    | StrOutputParser()\n)\n\nasync for chunk in retrieval_chain.astream_log(\n    \"where did harrison work?\", include_names=[\"Docs\"]\n):\n    print(\"-\" * 40)\n    print(chunk)\n\n----------------------------------------\nRunLogPatch({'op': 'replace',\n  'path': '',\n  'value': {'final_output': None,\n            'id': 'e2f2cc72-eb63-4d20-8326-237367482efb',\n            'logs': {},\n            'streamed_output': []}})\n----------------------------------------\nRunLogPatch({'op': 'add',\n  'path': '/logs/Docs',\n  'value': {'end_time': None,\n            'final_output': None,\n            'id': '8da492cc-4492-4e74-b8b0-9e60e8693390',\n            'metadata': {},\n            'name': 'Docs',\n            'start_time': '2023-10-19T17:50:13.526',\n            'streamed_output_str': [],\n            'tags': ['map:key:context', 'FAISS'],\n            'type': 'retriever'}})\n----------------------------------------\nRunLogPatch({'op': 'add',\n  'path': '/logs/Docs/final_output',\n  'value': {'documents': [Document(page_content='harrison worked at kensho')]}},\n {'op': 'add',\n  'path': '/logs/Docs/end_time',\n  'value': '2023-10-19T17:50:13.713'})\n----------------------------------------\nRunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ''})\n----------------------------------------\nRunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'H'})\n----------------------------------------\nRunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'arrison'})\n----------------------------------------\nRunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' worked'})\n----------------------------------------\nRunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' at'})\n----------------------------------------\nRunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' Kens'})\n----------------------------------------\nRunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'ho'})\n----------------------------------------\nRunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '.'})\n----------------------------------------\nRunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ''})\n----------------------------------------\nRunLogPatch({'op': 'replace',\n  'path': '/final_output',\n  'value': {'output': 'Harrison worked at Kensho.'}})\n\nStreaming the incremental RunState‚Äã\n\nYou can simply pass diff=False to get incremental values of RunState. You get more verbose output with more repetitive parts.\n\nasync for chunk in retrieval_chain.astream_log(\n    \"where did harrison work?\", include_names=[\"Docs\"], diff=False\n):\n    print(\"-\" * 70)\n    print(chunk)\n\n----------------------------------------------------------------------\nRunLog({'final_output': None,\n 'id': 'afe66178-d75f-4c2d-b348-b1d144239cd6',\n 'logs': {},\n 'streamed_output': []})\n----------------------------------------------------------------------\nRunLog({'final_output': None,\n 'id': 'afe66178-d75f-4c2d-b348-b1d144239cd6',\n 'logs': {'Docs': {'end_time': None,\n                   'final_output': None,\n                   'id': '88d51118-5756-4891-89c5-2f6a5e90cc26',\n                   'metadata': {},\n                   'name': 'Docs',\n                   'start_time': '2023-10-19T17:52:15.438',\n                   'streamed_output_str': [],\n                   'tags': ['map:key:context', 'FAISS'],\n                   'type': 'retriever'}},\n 'streamed_output': []})\n----------------------------------------------------------------------\nRunLog({'final_output': None,\n 'id': 'afe66178-d75f-4c2d-b348-b1d144239cd6',\n 'logs': {'Docs': {'end_time': '2023-10-19T17:52:15.738',\n                   'final_output': {'documents': [Document(page_content='harrison worked at kensho')]},\n                   'id': '88d51118-5756-4891-89c5-2f6a5e90cc26',\n                   'metadata': {},\n                   'name': 'Docs',\n                   'start_time': '2023-10-19T17:52:15.438',\n                   'streamed_output_str': [],\n                   'tags': ['map:key:context', 'FAISS'],\n                   'type': 'retriever'}},\n 'streamed_output': []})\n----------------------------------------------------------------------\nRunLog({'final_output': None,\n 'id': 'afe66178-d75f-4c2d-b348-b1d144239cd6',\n 'logs': {'Docs': {'end_time': '2023-10-19T17:52:15.738',\n                   'final_output': {'documents': [Document(page_content='harrison worked at kensho')]},\n                   'id': '88d51118-5756-4891-89c5-2f6a5e90cc26',\n                   'metadata': {},\n                   'name': 'Docs',\n                   'start_time': '2023-10-19T17:52:15.438',\n                   'streamed_output_str': [],\n                   'tags': ['map:key:context', 'FAISS'],\n                   'type': 'retriever'}},\n 'streamed_output': ['']})\n----------------------------------------------------------------------\nRunLog({'final_output': None,\n 'id': 'afe66178-d75f-4c2d-b348-b1d144239cd6',\n 'logs': {'Docs': {'end_time': '2023-10-19T17:52:15.738',\n                   'final_output': {'documents': [Document(page_content='harrison worked at kensho')]},\n                   'id': '88d51118-5756-4891-89c5-2f6a5e90cc26',\n                   'metadata': {},\n                   'name': 'Docs',\n                   'start_time': '2023-10-19T17:52:15.438',\n                   'streamed_output_str': [],\n                   'tags': ['map:key:context', 'FAISS'],\n                   'type': 'retriever'}},\n 'streamed_output': ['', 'H']})\n----------------------------------------------------------------------\nRunLog({'final_output': None,\n 'id': 'afe66178-d75f-4c2d-b348-b1d144239cd6',\n 'logs': {'Docs': {'end_time': '2023-10-19T17:52:15.738',\n                   'final_output': {'documents': [Document(page_content='harrison worked at kensho')]},\n                   'id': '88d51118-5756-4891-89c5-2f6a5e90cc26',\n                   'metadata': {},\n                   'name': 'Docs',\n                   'start_time': '2023-10-19T17:52:15.438',\n                   'streamed_output_str': [],\n                   'tags': ['map:key:context', 'FAISS'],\n                   'type': 'retriever'}},\n 'streamed_output': ['', 'H', 'arrison']})\n----------------------------------------------------------------------\nRunLog({'final_output': None,\n 'id': 'afe66178-d75f-4c2d-b348-b1d144239cd6',\n 'logs': {'Docs': {'end_time': '2023-10-19T17:52:15.738',\n                   'final_output': {'documents': [Document(page_content='harrison worked at kensho')]},\n                   'id': '88d51118-5756-4891-89c5-2f6a5e90cc26',\n                   'metadata': {},\n                   'name': 'Docs',\n                   'start_time': '2023-10-19T17:52:15.438',\n                   'streamed_output_str': [],\n                   'tags': ['map:key:context', 'FAISS'],\n                   'type': 'retriever'}},\n 'streamed_output': ['', 'H', 'arrison', ' worked']})\n----------------------------------------------------------------------\nRunLog({'final_output': None,\n 'id': 'afe66178-d75f-4c2d-b348-b1d144239cd6',\n 'logs': {'Docs': {'end_time': '2023-10-19T17:52:15.738',\n                   'final_output': {'documents': [Document(page_content='harrison worked at kensho')]},\n                   'id': '88d51118-5756-4891-89c5-2f6a5e90cc26',\n                   'metadata': {},\n                   'name': 'Docs',\n                   'start_time': '2023-10-19T17:52:15.438',\n                   'streamed_output_str': [],\n                   'tags': ['map:key:context', 'FAISS'],\n                   'type': 'retriever'}},\n 'streamed_output': ['', 'H', 'arrison', ' worked', ' at']})\n----------------------------------------------------------------------\nRunLog({'final_output': None,\n 'id': 'afe66178-d75f-4c2d-b348-b1d144239cd6',\n 'logs': {'Docs': {'end_time': '2023-10-19T17:52:15.738',\n                   'final_output': {'documents': [Document(page_content='harrison worked at kensho')]},\n                   'id': '88d51118-5756-4891-89c5-2f6a5e90cc26',\n                   'metadata': {},\n                   'name': 'Docs',\n                   'start_time': '2023-10-19T17:52:15.438',\n                   'streamed_output_str': [],\n                   'tags': ['map:key:context', 'FAISS'],\n                   'type': 'retriever'}},\n 'streamed_output': ['', 'H', 'arrison', ' worked', ' at', ' Kens']})\n----------------------------------------------------------------------\nRunLog({'final_output': None,\n 'id': 'afe66178-d75f-4c2d-b348-b1d144239cd6',\n 'logs': {'Docs': {'end_time': '2023-10-19T17:52:15.738',\n                   'final_output': {'documents': [Document(page_content='harrison worked at kensho')]},\n                   'id': '88d51118-5756-4891-89c5-2f6a5e90cc26',\n                   'metadata': {},\n                   'name': 'Docs',\n                   'start_time': '2023-10-19T17:52:15.438',\n                   'streamed_output_str': [],\n                   'tags': ['map:key:context', 'FAISS'],\n                   'type': 'retriever'}},\n 'streamed_output': ['', 'H', 'arrison', ' worked', ' at', ' Kens', 'ho']})\n----------------------------------------------------------------------\nRunLog({'final_output': None,\n 'id': 'afe66178-d75f-4c2d-b348-b1d144239cd6',\n 'logs': {'Docs': {'end_time': '2023-10-19T17:52:15.738',\n                   'final_output': {'documents': [Document(page_content='harrison worked at kensho')]},\n                   'id': '88d51118-5756-4891-89c5-2f6a5e90cc26',\n                   'metadata': {},\n                   'name': 'Docs',\n                   'start_time': '2023-10-19T17:52:15.438',\n                   'streamed_output_str': [],\n                   'tags': ['map:key:context', 'FAISS'],\n                   'type': 'retriever'}},\n 'streamed_output': ['', 'H', 'arrison', ' worked', ' at', ' Kens', 'ho', '.']})\n----------------------------------------------------------------------\nRunLog({'final_output': None,\n 'id': 'afe66178-d75f-4c2d-b348-b1d144239cd6',\n 'logs': {'Docs': {'end_time': '2023-10-19T17:52:15.738',\n                   'final_output': {'documents': [Document(page_content='harrison worked at kensho')]},\n                   'id': '88d51118-5756-4891-89c5-2f6a5e90cc26',\n                   'metadata': {},\n                   'name': 'Docs',\n                   'start_time': '2023-10-19T17:52:15.438',\n                   'streamed_output_str': [],\n                   'tags': ['map:key:context', 'FAISS'],\n                   'type': 'retriever'}},\n 'streamed_output': ['',\n                     'H',\n                     'arrison',\n                     ' worked',\n                     ' at',\n                     ' Kens',\n                     'ho',\n                     '.',\n                     '']})\n----------------------------------------------------------------------\nRunLog({'final_output': {'output': 'Harrison worked at Kensho.'},\n 'id': 'afe66178-d75f-4c2d-b348-b1d144239cd6',\n 'logs': {'Docs': {'end_time': '2023-10-19T17:52:15.738',\n                   'final_output': {'documents': [Document(page_content='harrison worked at kensho')]},\n                   'id': '88d51118-5756-4891-89c5-2f6a5e90cc26',\n                   'metadata': {},\n                   'name': 'Docs',\n                   'start_time': '2023-10-19T17:52:15.438',\n                   'streamed_output_str': [],\n                   'tags': ['map:key:context', 'FAISS'],\n                   'type': 'retriever'}},\n 'streamed_output': ['',\n                     'H',\n                     'arrison',\n                     ' worked',\n                     ' at',\n                     ' Kens',\n                     'ho',\n                     '.',\n                     '']})\n\nParallelism‚Äã\n\nLet‚Äôs take a look at how LangChain Expression Language supports parallel requests. For example, when using a RunnableParallel (often written as a dictionary) it executes each element in parallel.\n\nfrom langchain.schema.runnable import RunnableParallel\n\nchain1 = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\") | model\nchain2 = (\n    ChatPromptTemplate.from_template(\"write a short (2 line) poem about {topic}\")\n    | model\n)\ncombined = RunnableParallel(joke=chain1, poem=chain2)\n\n%%time\nchain1.invoke({\"topic\": \"bears\"})\n\nCPU times: user 54.3 ms, sys: 0 ns, total: 54.3 ms\nWall time: 2.29 s\n\nAIMessage(content=\"Why don't bears wear shoes?\\n\\nBecause they already have bear feet!\")\n\n%%time\nchain2.invoke({\"topic\": \"bears\"})\n\nCPU times: user 7.8 ms, sys: 0 ns, total: 7.8 ms\nWall time: 1.43 s\n\nAIMessage(content=\"In wild embrace,\\nNature's strength roams with grace.\")\n\n%%time\ncombined.invoke({\"topic\": \"bears\"})\n\nCPU times: user 167 ms, sys: 921 ¬µs, total: 168 ms\nWall time: 1.56 s\n\n{'joke': AIMessage(content=\"Why don't bears wear shoes?\\n\\nBecause they already have bear feet!\"),\n 'poem': AIMessage(content=\"Fierce and wild, nature's might,\\nBears roam the woods, shadows of the night.\")}\n\nParallelism on batches‚Äã\n\nParallelism can be combined with other runnables. Let‚Äôs try to use parallelism with batches.\n\n%%time\nchain1.batch([{\"topic\": \"bears\"}, {\"topic\": \"cats\"}])\n\nCPU times: user 159 ms, sys: 3.66 ms, total: 163 ms\nWall time: 1.34 s\n\n[AIMessage(content=\"Why don't bears wear shoes?\\n\\nBecause they already have bear feet!\"),\n AIMessage(content=\"Sure, here's a cat joke for you:\\n\\nWhy don't cats play poker in the wild?\\n\\nBecause there are too many cheetahs!\")]\n\n%%time\nchain2.batch([{\"topic\": \"bears\"}, {\"topic\": \"cats\"}])\n\nCPU times: user 165 ms, sys: 0 ns, total: 165 ms\nWall time: 1.73 s\n\n[AIMessage(content=\"Silent giants roam,\\nNature's strength, love's emblem shown.\"),\n AIMessage(content='Whiskers aglow, paws tiptoe,\\nGraceful hunters, hearts aglow.')]\n\n%%time\ncombined.batch([{\"topic\": \"bears\"}, {\"topic\": \"cats\"}])\n\nCPU times: user 507 ms, sys: 125 ms, total: 632 ms\nWall time: 1.49 s\n\n[{'joke': AIMessage(content=\"Why don't bears wear shoes?\\n\\nBecause they already have bear feet!\"),\n  'poem': AIMessage(content=\"Majestic bears roam,\\nNature's wild guardians of home.\")},\n {'joke': AIMessage(content=\"Sure, here's a cat joke for you:\\n\\nWhy did the cat sit on the computer?\\n\\nBecause it wanted to keep an eye on the mouse!\"),\n  'poem': AIMessage(content='Whiskers twitch, eyes gleam,\\nGraceful creatures, feline dream.')}]\n\nPrevious\nWhy use LCEL\nNext\nHow to\nInput Schema\nOutput Schema\nStream\nInvoke\nBatch\nAsync Stream\nAsync Invoke\nAsync Batch\nAsync Stream Intermediate Steps\nStreaming JSONPatch chunks\nStreaming the incremental RunState\nParallelism\nParallelism on batches\nCommunity\nDiscord\nTwitter\nGitHub\nPython\nJS/TS\nMore\nHomepage\nBlog\nCopyright ¬© 2023 LangChain, Inc."
}