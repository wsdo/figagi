{
	"title": "Why use LCEL | 🦜️🔗 Langchain",
	"url": "https://python.langchain.com/docs/expression_language/why",
	"html": "Skip to main content\n🦜️🔗 LangChain\nDocs\nUse cases\nIntegrations\nGuides\nAPI\nMore\n🦜️🔗\nChat\nSearch\n⌘\nK\nGet started\nIntroduction\nInstallation\nQuickstart\nSecurity\nLangChain Expression Language\nGet started\nWhy use LCEL\nInterface\nHow to\nCookbook\nModules\nModel I/O\nRetrieval\nAgents\nMore\nLangServe\nLangSmith\nLangChain Expression LanguageWhy use LCEL\nWhy use LCEL\n\nWe recommend reading the LCEL Get started section first.\n\nLCEL makes it easy to build complex chains from basic components. It does this by providing: 1. A unified interface: Every LCEL object implements the Runnable interface, which defines a common set of invocation methods (invoke, batch, stream, ainvoke, …). This makes it possible for chains of LCEL objects to also automatically support these invocations. That is, every chain of LCEL objects is itself an LCEL object. 2. Composition primitives: LCEL provides a number of primitives that make it easy to compose chains, parallelize components, add fallbacks, dynamically configure chain internal, and more.\n\nTo better understand the value of LCEL, it’s helpful to see it in action and think about how we might recreate similar functionality without it. In this walkthrough we’ll do just that with our basic example from the get started section. We’ll take our simple prompt + model chain, which under the hood already defines a lot of functionality, and see what it would take to recreate all of it.\n\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain.schema.output_parser import StrOutputParser\n\n\nprompt = ChatPromptTemplate.from_template(\"Tell me a short joke about {topic}\")\nmodel = ChatOpenAI(model=\"gpt-3.5-turbo\")\noutput_parser = StrOutputParser()\n\nchain = prompt | model | output_parser\n\nInvoke​\n\nIn the simplest case, we just want to pass in a topic string and get back a joke string:\n\nWithout LCEL​\n\nfrom typing import List\n\nimport openai\n\n\nprompt_template = \"Tell me a short joke about {topic}\"\nclient = openai.OpenAI()\n\ndef call_chat_model(messages: List[dict]) -> str:\n    response = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\", \n        messages=messages,\n    )\n    return response.choices[0].message.content\n\ndef invoke_chain(topic: str) -> str:\n    prompt_value = prompt_template.format(topic=topic)\n    messages = [{\"role\": \"user\", \"content\": prompt_value}]\n    return call_chat_model(messages)\n\ninvoke_chain(\"ice cream\")\n\nLCEL​\nfrom langchain_core.runnables import RunnablePassthrough\n\n\nprompt = ChatPromptTemplate.from_template(\n    \"Tell me a short joke about {topic}\"\n)\noutput_parser = StrOutputParser()\nmodel = ChatOpenAI(model=\"gpt-3.5-turbo\")\nchain = (\n    {\"topic\": RunnablePassthrough()} \n    | prompt\n    | model\n    | output_parser\n)\n\nchain.invoke(\"ice cream\")\n\nStream​\n\nIf we want to stream results instead, we’ll need to change our function:\n\nWithout LCEL​\nfrom typing import Iterator\n\n\ndef stream_chat_model(messages: List[dict]) -> Iterator[str]:\n    stream = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=messages,\n        stream=True,\n    )\n    for response in stream:\n        content = response.choices[0].delta.content\n        if content is not None:\n            yield content\n\ndef stream_chain(topic: str) -> Iterator[str]:\n    prompt_value = prompt.format(topic=topic)\n    return stream_chat_model([{\"role\": \"user\", \"content\": prompt_value}])\n\n\nfor chunk in stream_chain(\"ice cream\"):\n    print(chunk, end=\"\", flush=True)\n\nLCEL​\nfor chunk in chain.stream(\"ice cream\"):\n    print(chunk, end=\"\", flush=True)\n\nBatch​\n\nIf we want to run on a batch of inputs in parallel, we’ll again need a new function:\n\nWithout LCEL​\nfrom concurrent.futures import ThreadPoolExecutor\n\n\ndef batch_chain(topics: list) -> list:\n    with ThreadPoolExecutor(max_workers=5) as executor:\n        return list(executor.map(invoke_chain, topics))\n\nbatch_chain([\"ice cream\", \"spaghetti\", \"dumplings\"])\n\nLCEL​\nchain.batch([\"ice cream\", \"spaghetti\", \"dumplings\"])\n\nAsync​\n\nIf we need an asynchronous version:\n\nWithout LCEL​\nasync_client = openai.AsyncOpenAI()\n\nasync def acall_chat_model(messages: List[dict]) -> str:\n    response = await async_client.chat.completions.create(\n        model=\"gpt-3.5-turbo\", \n        messages=messages,\n    )\n    return response.choices[0].message.content\n\nasync def ainvoke_chain(topic: str) -> str:\n    prompt_value = prompt_template.format(topic=topic)\n    messages = [{\"role\": \"user\", \"content\": prompt_value}]\n    return await acall_chat_model(messages)\n\nawait ainvoke_chain(\"ice cream\")\n\nLCEL​\nchain.ainvoke(\"ice cream\")\n\nLLM instead of chat model​\n\nIf we want to use a completion endpoint instead of a chat endpoint:\n\nWithout LCEL​\ndef call_llm(prompt_value: str) -> str:\n    response = client.completions.create(\n        model=\"gpt-3.5-turbo-instruct\",\n        prompt=prompt_value,\n    )\n    return response.choices[0].text\n\ndef invoke_llm_chain(topic: str) -> str:\n    prompt_value = prompt_template.format(topic=topic)\n    return call_llm(prompt_value)\n\ninvoke_llm_chain(\"ice cream\")\n\nLCEL​\nfrom langchain.llms import OpenAI\n\nllm = OpenAI(model=\"gpt-3.5-turbo-instruct\")\nllm_chain = (\n    {\"topic\": RunnablePassthrough()} \n    | prompt\n    | llm\n    | output_parser\n)\n\nllm_chain.invoke(\"ice cream\")\n\nDifferent model provider​\n\nIf we want to use Anthropic instead of OpenAI:\n\nWithout LCEL​\nimport anthropic\n\nanthropic_template = f\"Human:\\n\\n{prompt_template}\\n\\nAssistant:\"\nanthropic_client = anthropic.Anthropic()\n\ndef call_anthropic(prompt_value: str) -> str:\n    response = anthropic_client.completions.create(\n        model=\"claude-2\",\n        prompt=prompt_value,\n        max_tokens_to_sample=256,\n    )\n    return response.completion    \n\ndef invoke_anthropic_chain(topic: str) -> str:\n    prompt_value = anthropic_template.format(topic=topic)\n    return call_anthropic(prompt_value)\n\ninvoke_anthropic_chain(\"ice cream\")\n\nLCEL​\nfrom langchain.chat_models import ChatAnthropic\n\nanthropic = ChatAnthropic(model=\"claude-2\")\nanthropic_chain = (\n    {\"topic\": RunnablePassthrough()} \n    | prompt \n    | anthropic\n    | output_parser\n)\n\nanthropic_chain.invoke(\"ice cream\")\n\nRuntime configurability​\n\nIf we wanted to make the choice of chat model or LLM configurable at runtime:\n\nWithout LCEL​\ndef invoke_configurable_chain(\n    topic: str, \n    *, \n    model: str = \"chat_openai\"\n) -> str:\n    if model == \"chat_openai\":\n        return invoke_chain(topic)\n    elif model == \"openai\":\n        return invoke_llm_chain(topic)\n    elif model == \"anthropic\":\n        return invoke_anthropic_chain(topic)\n    else:\n        raise ValueError(\n            f\"Received invalid model '{model}'.\"\n            \" Expected one of chat_openai, openai, anthropic\"\n        )\n\ndef stream_configurable_chain(\n    topic: str, \n    *, \n    model: str = \"chat_openai\"\n) -> Iterator[str]:\n    if model == \"chat_openai\":\n        return stream_chain(topic)\n    elif model == \"openai\":\n        # Note we haven't implemented this yet.\n        return stream_llm_chain(topic)\n    elif model == \"anthropic\":\n        # Note we haven't implemented this yet\n        return stream_anthropic_chain(topic)\n    else:\n        raise ValueError(\n            f\"Received invalid model '{model}'.\"\n            \" Expected one of chat_openai, openai, anthropic\"\n        )\n\ndef batch_configurable_chain(\n    topics: List[str], \n    *, \n    model: str = \"chat_openai\"\n) -> List[str]:\n    # You get the idea\n    ...\n\nasync def abatch_configurable_chain(\n    topics: List[str], \n    *, \n    model: str = \"chat_openai\"\n) -> List[str]:\n    ...\n\ninvoke_configurable_chain(\"ice cream\", model=\"openai\")\nstream = stream_configurable_chain(\n    \"ice_cream\", \n    model=\"anthropic\"\n)\nfor chunk in stream:\n    print(chunk, end=\"\", flush=True)\n\n# batch_configurable_chain([\"ice cream\", \"spaghetti\", \"dumplings\"])\n# await ainvoke_configurable_chain(\"ice cream\")\n\nWith LCEL​\nfrom langchain_core.runnables import ConfigurableField\n\n\nconfigurable_model = model.configurable_alternatives(\n    ConfigurableField(id=\"model\"), \n    default_key=\"chat_openai\", \n    openai=llm,\n    anthropic=anthropic,\n)\nconfigurable_chain = (\n    {\"topic\": RunnablePassthrough()} \n    | prompt \n    | configurable_model \n    | output_parser\n)\n\nconfigurable_chain.invoke(\n    \"ice cream\", \n    config={\"model\": \"openai\"}\n)\nstream = configurable_chain.stream(\n    \"ice cream\", \n    config={\"model\": \"anthropic\"}\n)\nfor chunk in stream:\n    print(chunk, end=\"\", flush=True)\n\nconfigurable_chain.batch([\"ice cream\", \"spaghetti\", \"dumplings\"])\n\n# await configurable_chain.ainvoke(\"ice cream\")\n\nLogging​\n\nIf we want to log our intermediate results:\n\nWithout LCEL​\n\nWe’ll print intermediate steps for illustrative purposes\n\ndef invoke_anthropic_chain_with_logging(topic: str) -> str:\n    print(f\"Input: {topic}\")\n    prompt_value = anthropic_template.format(topic=topic)\n    print(f\"Formatted prompt: {prompt_value}\")\n    output = call_anthropic(prompt_value)\n    print(f\"Output: {output}\")\n    return output\n\ninvoke_anthropic_chain_with_logging(\"ice cream\")\n\nLCEL​\n\nEvery component has built-in integrations with LangSmith. If we set the following two environment variables, all chain traces are logged to LangSmith.\n\nimport os\n\nos.environ[\"LANGCHAIN_API_KEY\"] = \"...\"\nos.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n\nanthropic_chain.invoke(\"ice cream\")\n\n\nHere’s what our LangSmith trace looks like: https://smith.langchain.com/public/e4de52f8-bcd9-4732-b950-deee4b04e313/r\n\nFallbacks​\n\nIf we wanted to add fallback logic, in case one model API is down:\n\nWithout LCEL​\ndef invoke_chain_with_fallback(topic: str) -> str:\n    try:\n        return invoke_chain(topic)\n    except Exception:\n        return invoke_anthropic_chain(topic)\n\nasync def ainvoke_chain_with_fallback(topic: str) -> str:\n    try:\n        return await ainvoke_chain(topic)\n    except Exception:\n        # Note: we haven't actually implemented this.\n        return ainvoke_anthropic_chain(topic)\n\nasync def batch_chain_with_fallback(topics: List[str]) -> str:\n    try:\n        return batch_chain(topics)\n    except Exception:\n        # Note: we haven't actually implemented this.\n        return batch_anthropic_chain(topics)\n\ninvoke_chain_with_fallback(\"ice cream\")\n# await ainvoke_chain_with_fallback(\"ice cream\")\nbatch_chain_with_fallback([\"ice cream\", \"spaghetti\", \"dumplings\"]))\n\nLCEL​\nfallback_chain = chain.with_fallbacks([anthropic_chain])\n\nfallback_chain.invoke(\"ice cream\")\n# await fallback_chain.ainvoke(\"ice cream\")\nfallback_chain.batch([\"ice cream\", \"spaghetti\", \"dumplings\"])\n\nFull code comparison​\n\nEven in this simple case, our LCEL chain succinctly packs in a lot of functionality. As chains become more complex, this becomes especially valuable.\n\nWithout LCEL​\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import Iterator, List, Tuple\n\nimport anthropic\nimport openai\n\n\nprompt_template = \"Tell me a short joke about {topic}\"\nanthropic_template = f\"Human:\\n\\n{prompt_template}\\n\\nAssistant:\"\nclient = openai.OpenAI()\nasync_client = openai.AsyncOpenAI()\nanthropic_client = anthropic.Anthropic()\n\ndef call_chat_model(messages: List[dict]) -> str:\n    response = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\", \n        messages=messages,\n    )\n    return response.choices[0].message.content\n\ndef invoke_chain(topic: str) -> str:\n    print(f\"Input: {topic}\")\n    prompt_value = prompt_template.format(topic=topic)\n    print(f\"Formatted prompt: {prompt_value}\")\n    messages = [{\"role\": \"user\", \"content\": prompt_value}]\n    output = call_chat_model(messages)\n    print(f\"Output: {output}\")\n    return output\n\ndef stream_chat_model(messages: List[dict]) -> Iterator[str]:\n    stream = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=messages,\n        stream=True,\n    )\n    for response in stream:\n        content = response.choices[0].delta.content\n        if content is not None:\n            yield content\n\ndef stream_chain(topic: str) -> Iterator[str]:\n    print(f\"Input: {topic}\")\n    prompt_value = prompt.format(topic=topic)\n    print(f\"Formatted prompt: {prompt_value}\")\n    stream = stream_chat_model([{\"role\": \"user\", \"content\": prompt_value}])\n    for chunk in stream:\n        print(f\"Token: {chunk}\", end=\"\")\n        yield chunk\n\ndef batch_chain(topics: list) -> list:\n    with ThreadPoolExecutor(max_workers=5) as executor:\n        return list(executor.map(invoke_chain, topics))\n\ndef call_llm(prompt_value: str) -> str:\n    response = client.completions.create(\n        model=\"gpt-3.5-turbo-instruct\",\n        prompt=prompt_value,\n    )\n    return response.choices[0].text\n\ndef invoke_llm_chain(topic: str) -> str:\n    print(f\"Input: {topic}\")\n    prompt_value = promtp_template.format(topic=topic)\n    print(f\"Formatted prompt: {prompt_value}\")\n    output = call_llm(prompt_value)\n    print(f\"Output: {output}\")\n    return output\n\ndef call_anthropic(prompt_value: str) -> str:\n    response = anthropic_client.completions.create(\n        model=\"claude-2\",\n        prompt=prompt_value,\n        max_tokens_to_sample=256,\n    )\n    return response.completion   \n\ndef invoke_anthropic_chain(topic: str) -> str:\n    print(f\"Input: {topic}\")\n    prompt_value = anthropic_template.format(topic=topic)\n    print(f\"Formatted prompt: {prompt_value}\")\n    output = call_anthropic(prompt_value)\n    print(f\"Output: {output}\")\n    return output\n\nasync def ainvoke_anthropic_chain(topic: str) -> str:\n    ...\n\ndef stream_anthropic_chain(topic: str) -> Iterator[str]:\n    ...\n\ndef batch_anthropic_chain(topics: List[str]) -> List[str]:\n    ...\n\ndef invoke_configurable_chain(\n    topic: str, \n    *, \n    model: str = \"chat_openai\"\n) -> str:\n    if model == \"chat_openai\":\n        return invoke_chain(topic)\n    elif model == \"openai\":\n        return invoke_llm_chain(topic)\n    elif model == \"anthropic\":\n        return invoke_anthropic_chain(topic)\n    else:\n        raise ValueError(\n            f\"Received invalid model '{model}'.\"\n            \" Expected one of chat_openai, openai, anthropic\"\n        )\n\ndef stream_configurable_chain(\n    topic: str, \n    *, \n    model: str = \"chat_openai\"\n) -> Iterator[str]:\n    if model == \"chat_openai\":\n        return stream_chain(topic)\n    elif model == \"openai\":\n        # Note we haven't implemented this yet.\n        return stream_llm_chain(topic)\n    elif model == \"anthropic\":\n        # Note we haven't implemented this yet\n        return stream_anthropic_chain(topic)\n    else:\n        raise ValueError(\n            f\"Received invalid model '{model}'.\"\n            \" Expected one of chat_openai, openai, anthropic\"\n        )\n\ndef batch_configurable_chain(\n    topics: List[str], \n    *, \n    model: str = \"chat_openai\"\n) -> List[str]:\n    ...\n\nasync def abatch_configurable_chain(\n    topics: List[str], \n    *, \n    model: str = \"chat_openai\"\n) -> List[str]:\n    ...\n\ndef invoke_chain_with_fallback(topic: str) -> str:\n    try:\n        return invoke_chain(topic)\n    except Exception:\n        return invoke_anthropic_chain(topic)\n\nasync def ainvoke_chain_with_fallback(topic: str) -> str:\n    try:\n        return await ainvoke_chain(topic)\n    except Exception:\n        return ainvoke_anthropic_chain(topic)\n\nasync def batch_chain_with_fallback(topics: List[str]) -> str:\n    try:\n        return batch_chain(topics)\n    except Exception:\n        return batch_anthropic_chain(topics)\n\nLCEL​\nimport os\n\nfrom langchain.chat_models import ChatAnthropic, ChatOpenAI\nfrom langchain.llms import OpenAI\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.runnables import RunnablePassthrough\n\nos.environ[\"LANGCHAIN_API_KEY\"] = \"...\"\nos.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n\nprompt = ChatPromptTemplate.from_template(\n    \"Tell me a short joke about {topic}\"\n)\nchat_openai = ChatOpenAI(model=\"gpt-3.5-turbo\")\nopenai = OpenAI(model=\"gpt-3.5-turbo-instruct\")\nanthropic = ChatAnthropic(model=\"claude-2\")\nmodel = (\n    chat_openai\n    .with_fallbacks([anthropic])\n    .configurable_alternatives(\n        ConfigurableField(id=\"model\"),\n        default_key=\"chat_openai\",\n        openai=openai,\n        anthropic=anthropic,\n    )\n)\n\nchain = (\n    {\"topic\": RunnablePassthrough()} \n    | prompt \n    | model \n    | StrOutputParser()\n)\n\nNext steps​\n\nTo continue learning about LCEL, we recommend: - Reading up on the full LCEL Interface, which we’ve only partially covered here. - Exploring the How-to section to learn about additional composition primitives that LCEL provides. - Looking through the Cookbook section to see LCEL in action for common use cases. A good next use case to look at would be Retrieval-augmented generation.\n\nPrevious\nGet started\nNext\nInterface\nInvoke\nStream\nBatch\nAsync\nLLM instead of chat model\nDifferent model provider\nRuntime configurability\nLogging\nFallbacks\nFull code comparison\nNext steps\nCommunity\nDiscord\nTwitter\nGitHub\nPython\nJS/TS\nMore\nHomepage\nBlog\nCopyright © 2023 LangChain, Inc."
}