{
	"title": "Why use LCEL | ðŸ¦œï¸ðŸ”— Langchain",
	"url": "https://python.langchain.com/docs/expression_language/why",
	"html": "Skip to main content\nðŸ¦œï¸ðŸ”— LangChain\nDocs\nUse cases\nIntegrations\nGuides\nAPI\nMore\nðŸ¦œï¸ðŸ”—\nChat\nSearch\nâŒ˜\nK\nGet started\nIntroduction\nInstallation\nQuickstart\nSecurity\nLangChain Expression Language\nGet started\nWhy use LCEL\nInterface\nHow to\nCookbook\nModules\nModel I/O\nRetrieval\nAgents\nMore\nLangServe\nLangSmith\nLangChain Expression LanguageWhy use LCEL\nWhy use LCEL\n\nWe recommend reading the LCEL Get started section first.\n\nLCEL makes it easy to build complex chains from basic components. It does this by providing: 1. A unified interface: Every LCEL object implements the Runnable interface, which defines a common set of invocation methods (invoke, batch, stream, ainvoke, â€¦). This makes it possible for chains of LCEL objects to also automatically support these invocations. That is, every chain of LCEL objects is itself an LCEL object. 2. Composition primitives: LCEL provides a number of primitives that make it easy to compose chains, parallelize components, add fallbacks, dynamically configure chain internal, and more.\n\nTo better understand the value of LCEL, itâ€™s helpful to see it in action and think about how we might recreate similar functionality without it. In this walkthrough weâ€™ll do just that with our basic example from the get started section. Weâ€™ll take our simple prompt + model chain, which under the hood already defines a lot of functionality, and see what it would take to recreate all of it.\n\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain.schema.output_parser import StrOutputParser\n\n\nprompt = ChatPromptTemplate.from_template(\"Tell me a short joke about {topic}\")\nmodel = ChatOpenAI(model=\"gpt-3.5-turbo\")\noutput_parser = StrOutputParser()\n\nchain = prompt | model | output_parser\n\nInvokeâ€‹\n\nIn the simplest case, we just want to pass in a topic string and get back a joke string:\n\nWithout LCELâ€‹\n\nfrom typing import List\n\nimport openai\n\n\nprompt_template = \"Tell me a short joke about {topic}\"\nclient = openai.OpenAI()\n\ndef call_chat_model(messages: List[dict]) -> str:\n    response = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\", \n        messages=messages,\n    )\n    return response.choices[0].message.content\n\ndef invoke_chain(topic: str) -> str:\n    prompt_value = prompt_template.format(topic=topic)\n    messages = [{\"role\": \"user\", \"content\": prompt_value}]\n    return call_chat_model(messages)\n\ninvoke_chain(\"ice cream\")\n\nLCELâ€‹\nfrom langchain_core.runnables import RunnablePassthrough\n\n\nprompt = ChatPromptTemplate.from_template(\n    \"Tell me a short joke about {topic}\"\n)\noutput_parser = StrOutputParser()\nmodel = ChatOpenAI(model=\"gpt-3.5-turbo\")\nchain = (\n    {\"topic\": RunnablePassthrough()} \n    | prompt\n    | model\n    | output_parser\n)\n\nchain.invoke(\"ice cream\")\n\nStreamâ€‹\n\nIf we want to stream results instead, weâ€™ll need to change our function:\n\nWithout LCELâ€‹\nfrom typing import Iterator\n\n\ndef stream_chat_model(messages: List[dict]) -> Iterator[str]:\n    stream = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=messages,\n        stream=True,\n    )\n    for response in stream:\n        content = response.choices[0].delta.content\n        if content is not None:\n            yield content\n\ndef stream_chain(topic: str) -> Iterator[str]:\n    prompt_value = prompt.format(topic=topic)\n    return stream_chat_model([{\"role\": \"user\", \"content\": prompt_value}])\n\n\nfor chunk in stream_chain(\"ice cream\"):\n    print(chunk, end=\"\", flush=True)\n\nLCELâ€‹\nfor chunk in chain.stream(\"ice cream\"):\n    print(chunk, end=\"\", flush=True)\n\nBatchâ€‹\n\nIf we want to run on a batch of inputs in parallel, weâ€™ll again need a new function:\n\nWithout LCELâ€‹\nfrom concurrent.futures import ThreadPoolExecutor\n\n\ndef batch_chain(topics: list) -> list:\n    with ThreadPoolExecutor(max_workers=5) as executor:\n        return list(executor.map(invoke_chain, topics))\n\nbatch_chain([\"ice cream\", \"spaghetti\", \"dumplings\"])\n\nLCELâ€‹\nchain.batch([\"ice cream\", \"spaghetti\", \"dumplings\"])\n\nAsyncâ€‹\n\nIf we need an asynchronous version:\n\nWithout LCELâ€‹\nasync_client = openai.AsyncOpenAI()\n\nasync def acall_chat_model(messages: List[dict]) -> str:\n    response = await async_client.chat.completions.create(\n        model=\"gpt-3.5-turbo\", \n        messages=messages,\n    )\n    return response.choices[0].message.content\n\nasync def ainvoke_chain(topic: str) -> str:\n    prompt_value = prompt_template.format(topic=topic)\n    messages = [{\"role\": \"user\", \"content\": prompt_value}]\n    return await acall_chat_model(messages)\n\nawait ainvoke_chain(\"ice cream\")\n\nLCELâ€‹\nchain.ainvoke(\"ice cream\")\n\nLLM instead of chat modelâ€‹\n\nIf we want to use a completion endpoint instead of a chat endpoint:\n\nWithout LCELâ€‹\ndef call_llm(prompt_value: str) -> str:\n    response = client.completions.create(\n        model=\"gpt-3.5-turbo-instruct\",\n        prompt=prompt_value,\n    )\n    return response.choices[0].text\n\ndef invoke_llm_chain(topic: str) -> str:\n    prompt_value = prompt_template.format(topic=topic)\n    return call_llm(prompt_value)\n\ninvoke_llm_chain(\"ice cream\")\n\nLCELâ€‹\nfrom langchain.llms import OpenAI\n\nllm = OpenAI(model=\"gpt-3.5-turbo-instruct\")\nllm_chain = (\n    {\"topic\": RunnablePassthrough()} \n    | prompt\n    | llm\n    | output_parser\n)\n\nllm_chain.invoke(\"ice cream\")\n\nDifferent model providerâ€‹\n\nIf we want to use Anthropic instead of OpenAI:\n\nWithout LCELâ€‹\nimport anthropic\n\nanthropic_template = f\"Human:\\n\\n{prompt_template}\\n\\nAssistant:\"\nanthropic_client = anthropic.Anthropic()\n\ndef call_anthropic(prompt_value: str) -> str:\n    response = anthropic_client.completions.create(\n        model=\"claude-2\",\n        prompt=prompt_value,\n        max_tokens_to_sample=256,\n    )\n    return response.completion    \n\ndef invoke_anthropic_chain(topic: str) -> str:\n    prompt_value = anthropic_template.format(topic=topic)\n    return call_anthropic(prompt_value)\n\ninvoke_anthropic_chain(\"ice cream\")\n\nLCELâ€‹\nfrom langchain.chat_models import ChatAnthropic\n\nanthropic = ChatAnthropic(model=\"claude-2\")\nanthropic_chain = (\n    {\"topic\": RunnablePassthrough()} \n    | prompt \n    | anthropic\n    | output_parser\n)\n\nanthropic_chain.invoke(\"ice cream\")\n\nRuntime configurabilityâ€‹\n\nIf we wanted to make the choice of chat model or LLM configurable at runtime:\n\nWithout LCELâ€‹\ndef invoke_configurable_chain(\n    topic: str, \n    *, \n    model: str = \"chat_openai\"\n) -> str:\n    if model == \"chat_openai\":\n        return invoke_chain(topic)\n    elif model == \"openai\":\n        return invoke_llm_chain(topic)\n    elif model == \"anthropic\":\n        return invoke_anthropic_chain(topic)\n    else:\n        raise ValueError(\n            f\"Received invalid model '{model}'.\"\n            \" Expected one of chat_openai, openai, anthropic\"\n        )\n\ndef stream_configurable_chain(\n    topic: str, \n    *, \n    model: str = \"chat_openai\"\n) -> Iterator[str]:\n    if model == \"chat_openai\":\n        return stream_chain(topic)\n    elif model == \"openai\":\n        # Note we haven't implemented this yet.\n        return stream_llm_chain(topic)\n    elif model == \"anthropic\":\n        # Note we haven't implemented this yet\n        return stream_anthropic_chain(topic)\n    else:\n        raise ValueError(\n            f\"Received invalid model '{model}'.\"\n            \" Expected one of chat_openai, openai, anthropic\"\n        )\n\ndef batch_configurable_chain(\n    topics: List[str], \n    *, \n    model: str = \"chat_openai\"\n) -> List[str]:\n    # You get the idea\n    ...\n\nasync def abatch_configurable_chain(\n    topics: List[str], \n    *, \n    model: str = \"chat_openai\"\n) -> List[str]:\n    ...\n\ninvoke_configurable_chain(\"ice cream\", model=\"openai\")\nstream = stream_configurable_chain(\n    \"ice_cream\", \n    model=\"anthropic\"\n)\nfor chunk in stream:\n    print(chunk, end=\"\", flush=True)\n\n# batch_configurable_chain([\"ice cream\", \"spaghetti\", \"dumplings\"])\n# await ainvoke_configurable_chain(\"ice cream\")\n\nWith LCELâ€‹\nfrom langchain_core.runnables import ConfigurableField\n\n\nconfigurable_model = model.configurable_alternatives(\n    ConfigurableField(id=\"model\"), \n    default_key=\"chat_openai\", \n    openai=llm,\n    anthropic=anthropic,\n)\nconfigurable_chain = (\n    {\"topic\": RunnablePassthrough()} \n    | prompt \n    | configurable_model \n    | output_parser\n)\n\nconfigurable_chain.invoke(\n    \"ice cream\", \n    config={\"model\": \"openai\"}\n)\nstream = configurable_chain.stream(\n    \"ice cream\", \n    config={\"model\": \"anthropic\"}\n)\nfor chunk in stream:\n    print(chunk, end=\"\", flush=True)\n\nconfigurable_chain.batch([\"ice cream\", \"spaghetti\", \"dumplings\"])\n\n# await configurable_chain.ainvoke(\"ice cream\")\n\nLoggingâ€‹\n\nIf we want to log our intermediate results:\n\nWithout LCELâ€‹\n\nWeâ€™ll print intermediate steps for illustrative purposes\n\ndef invoke_anthropic_chain_with_logging(topic: str) -> str:\n    print(f\"Input: {topic}\")\n    prompt_value = anthropic_template.format(topic=topic)\n    print(f\"Formatted prompt: {prompt_value}\")\n    output = call_anthropic(prompt_value)\n    print(f\"Output: {output}\")\n    return output\n\ninvoke_anthropic_chain_with_logging(\"ice cream\")\n\nLCELâ€‹\n\nEvery component has built-in integrations with LangSmith. If we set the following two environment variables, all chain traces are logged to LangSmith.\n\nimport os\n\nos.environ[\"LANGCHAIN_API_KEY\"] = \"...\"\nos.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n\nanthropic_chain.invoke(\"ice cream\")\n\n\nHereâ€™s what our LangSmith trace looks like: https://smith.langchain.com/public/e4de52f8-bcd9-4732-b950-deee4b04e313/r\n\nFallbacksâ€‹\n\nIf we wanted to add fallback logic, in case one model API is down:\n\nWithout LCELâ€‹\ndef invoke_chain_with_fallback(topic: str) -> str:\n    try:\n        return invoke_chain(topic)\n    except Exception:\n        return invoke_anthropic_chain(topic)\n\nasync def ainvoke_chain_with_fallback(topic: str) -> str:\n    try:\n        return await ainvoke_chain(topic)\n    except Exception:\n        # Note: we haven't actually implemented this.\n        return ainvoke_anthropic_chain(topic)\n\nasync def batch_chain_with_fallback(topics: List[str]) -> str:\n    try:\n        return batch_chain(topics)\n    except Exception:\n        # Note: we haven't actually implemented this.\n        return batch_anthropic_chain(topics)\n\ninvoke_chain_with_fallback(\"ice cream\")\n# await ainvoke_chain_with_fallback(\"ice cream\")\nbatch_chain_with_fallback([\"ice cream\", \"spaghetti\", \"dumplings\"]))\n\nLCELâ€‹\nfallback_chain = chain.with_fallbacks([anthropic_chain])\n\nfallback_chain.invoke(\"ice cream\")\n# await fallback_chain.ainvoke(\"ice cream\")\nfallback_chain.batch([\"ice cream\", \"spaghetti\", \"dumplings\"])\n\nFull code comparisonâ€‹\n\nEven in this simple case, our LCEL chain succinctly packs in a lot of functionality. As chains become more complex, this becomes especially valuable.\n\nWithout LCELâ€‹\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import Iterator, List, Tuple\n\nimport anthropic\nimport openai\n\n\nprompt_template = \"Tell me a short joke about {topic}\"\nanthropic_template = f\"Human:\\n\\n{prompt_template}\\n\\nAssistant:\"\nclient = openai.OpenAI()\nasync_client = openai.AsyncOpenAI()\nanthropic_client = anthropic.Anthropic()\n\ndef call_chat_model(messages: List[dict]) -> str:\n    response = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\", \n        messages=messages,\n    )\n    return response.choices[0].message.content\n\ndef invoke_chain(topic: str) -> str:\n    print(f\"Input: {topic}\")\n    prompt_value = prompt_template.format(topic=topic)\n    print(f\"Formatted prompt: {prompt_value}\")\n    messages = [{\"role\": \"user\", \"content\": prompt_value}]\n    output = call_chat_model(messages)\n    print(f\"Output: {output}\")\n    return output\n\ndef stream_chat_model(messages: List[dict]) -> Iterator[str]:\n    stream = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=messages,\n        stream=True,\n    )\n    for response in stream:\n        content = response.choices[0].delta.content\n        if content is not None:\n            yield content\n\ndef stream_chain(topic: str) -> Iterator[str]:\n    print(f\"Input: {topic}\")\n    prompt_value = prompt.format(topic=topic)\n    print(f\"Formatted prompt: {prompt_value}\")\n    stream = stream_chat_model([{\"role\": \"user\", \"content\": prompt_value}])\n    for chunk in stream:\n        print(f\"Token: {chunk}\", end=\"\")\n        yield chunk\n\ndef batch_chain(topics: list) -> list:\n    with ThreadPoolExecutor(max_workers=5) as executor:\n        return list(executor.map(invoke_chain, topics))\n\ndef call_llm(prompt_value: str) -> str:\n    response = client.completions.create(\n        model=\"gpt-3.5-turbo-instruct\",\n        prompt=prompt_value,\n    )\n    return response.choices[0].text\n\ndef invoke_llm_chain(topic: str) -> str:\n    print(f\"Input: {topic}\")\n    prompt_value = promtp_template.format(topic=topic)\n    print(f\"Formatted prompt: {prompt_value}\")\n    output = call_llm(prompt_value)\n    print(f\"Output: {output}\")\n    return output\n\ndef call_anthropic(prompt_value: str) -> str:\n    response = anthropic_client.completions.create(\n        model=\"claude-2\",\n        prompt=prompt_value,\n        max_tokens_to_sample=256,\n    )\n    return response.completion   \n\ndef invoke_anthropic_chain(topic: str) -> str:\n    print(f\"Input: {topic}\")\n    prompt_value = anthropic_template.format(topic=topic)\n    print(f\"Formatted prompt: {prompt_value}\")\n    output = call_anthropic(prompt_value)\n    print(f\"Output: {output}\")\n    return output\n\nasync def ainvoke_anthropic_chain(topic: str) -> str:\n    ...\n\ndef stream_anthropic_chain(topic: str) -> Iterator[str]:\n    ...\n\ndef batch_anthropic_chain(topics: List[str]) -> List[str]:\n    ...\n\ndef invoke_configurable_chain(\n    topic: str, \n    *, \n    model: str = \"chat_openai\"\n) -> str:\n    if model == \"chat_openai\":\n        return invoke_chain(topic)\n    elif model == \"openai\":\n        return invoke_llm_chain(topic)\n    elif model == \"anthropic\":\n        return invoke_anthropic_chain(topic)\n    else:\n        raise ValueError(\n            f\"Received invalid model '{model}'.\"\n            \" Expected one of chat_openai, openai, anthropic\"\n        )\n\ndef stream_configurable_chain(\n    topic: str, \n    *, \n    model: str = \"chat_openai\"\n) -> Iterator[str]:\n    if model == \"chat_openai\":\n        return stream_chain(topic)\n    elif model == \"openai\":\n        # Note we haven't implemented this yet.\n        return stream_llm_chain(topic)\n    elif model == \"anthropic\":\n        # Note we haven't implemented this yet\n        return stream_anthropic_chain(topic)\n    else:\n        raise ValueError(\n            f\"Received invalid model '{model}'.\"\n            \" Expected one of chat_openai, openai, anthropic\"\n        )\n\ndef batch_configurable_chain(\n    topics: List[str], \n    *, \n    model: str = \"chat_openai\"\n) -> List[str]:\n    ...\n\nasync def abatch_configurable_chain(\n    topics: List[str], \n    *, \n    model: str = \"chat_openai\"\n) -> List[str]:\n    ...\n\ndef invoke_chain_with_fallback(topic: str) -> str:\n    try:\n        return invoke_chain(topic)\n    except Exception:\n        return invoke_anthropic_chain(topic)\n\nasync def ainvoke_chain_with_fallback(topic: str) -> str:\n    try:\n        return await ainvoke_chain(topic)\n    except Exception:\n        return ainvoke_anthropic_chain(topic)\n\nasync def batch_chain_with_fallback(topics: List[str]) -> str:\n    try:\n        return batch_chain(topics)\n    except Exception:\n        return batch_anthropic_chain(topics)\n\nLCELâ€‹\nimport os\n\nfrom langchain.chat_models import ChatAnthropic, ChatOpenAI\nfrom langchain.llms import OpenAI\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.runnables import RunnablePassthrough\n\nos.environ[\"LANGCHAIN_API_KEY\"] = \"...\"\nos.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n\nprompt = ChatPromptTemplate.from_template(\n    \"Tell me a short joke about {topic}\"\n)\nchat_openai = ChatOpenAI(model=\"gpt-3.5-turbo\")\nopenai = OpenAI(model=\"gpt-3.5-turbo-instruct\")\nanthropic = ChatAnthropic(model=\"claude-2\")\nmodel = (\n    chat_openai\n    .with_fallbacks([anthropic])\n    .configurable_alternatives(\n        ConfigurableField(id=\"model\"),\n        default_key=\"chat_openai\",\n        openai=openai,\n        anthropic=anthropic,\n    )\n)\n\nchain = (\n    {\"topic\": RunnablePassthrough()} \n    | prompt \n    | model \n    | StrOutputParser()\n)\n\nNext stepsâ€‹\n\nTo continue learning about LCEL, we recommend: - Reading up on the full LCEL Interface, which weâ€™ve only partially covered here. - Exploring the How-to section to learn about additional composition primitives that LCEL provides. - Looking through the Cookbook section to see LCEL in action for common use cases. A good next use case to look at would be Retrieval-augmented generation.\n\nPrevious\nGet started\nNext\nInterface\nInvoke\nStream\nBatch\nAsync\nLLM instead of chat model\nDifferent model provider\nRuntime configurability\nLogging\nFallbacks\nFull code comparison\nNext steps\nCommunity\nDiscord\nTwitter\nGitHub\nPython\nJS/TS\nMore\nHomepage\nBlog\nCopyright Â© 2023 LangChain, Inc."
}